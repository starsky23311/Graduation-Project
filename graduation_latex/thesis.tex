% !Mode:: "TeX:UTF-8"
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%          ,
%      /\^/`\
%     | \/   |                CONGRATULATIONS!
%     | |    |             SPRING IS IN THE AIR!
%     \ \    /                                                _ _
%      '\\//'                                               _{ ' }_
%        ||                     hithesis v3                { `.!.` }
%        ||                                                ',_/Y\_,'
%        ||  ,                   dustincys                   {_,_}
%    |\  ||  |\          Email: yanshuoc@gmail.com             |
%    | | ||  | |            https://yanshuo.name             (\|  /)
%    | | || / /                                               \| //
%    \ \||/ /       https://github.com/dustincys/hithesis      |//
%      `\\//`   \\   \./    \\ /     //    \\./   \\   //   \\ |/ /
%     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[fontset=fandol,type=bachelor,campus=harbin]{hithesisbook}
% 此处选项中不要有空格
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 必填选项
% type=doctor|master|bachelor|postdoc
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 选填选项（选填选项的缺省值已经尽可能满足了大多数需求，除非明确知道自己有什么
% 需求）
% campus=shenzhen|weihai|harbin
%   含义：校区选项，默认harbin
% glue=true|false
%   含义：由于我工规范中要求字体行距在一个闭区间内，这个选项为true表示tex自
%   动选择，为false表示区间内一个最接近版心要求行数的要求的默认值，缺省值为
%   false。
% tocfour=true|false
%   含义：是否添加第四级目录，只对本科文科个别要求四级目录有效，缺省值为
%   false
% fontset=windows|mac|ubuntu|fandol|adobe
%   含义：设置字体，默认情况会自动识别系统，然后设置字体。后两个是开源字体，自行
%   下载安装后设置使用。windows是中易字库，窝工默认常用字体，绝对没毛病。mac和
%   ubuntu 默认分别是华文和思源字库，理论上用什么字库都行。后两种开源字库的安装
%   方法到谷歌上百度一下什么都有了。Linux非ubuntu发行版、非x86架构机器等如何运行
%   可到github issue上讨论。
% tocblank=true|false
%   含义：目录中第一章之前，是否加一行空白。缺省值为true。
% chapterhang=true|false
%   含义：目录的章标题是否悬挂居中，规范中要求章标题少于15字，所以这个选项
%   有无没什么用，除了特殊需求。缺省值为true。
% fulltime=true|false
%   含义：是否全日制，缺省值为true。非全日制如同等学力等，要在cover中设置类
%   型，封面中不同格式
% subtitle=true|false
%   含义：论文题目是否含有副标题，缺省值为false，如果有要在cover中设置副标
%   题内容，封面中显示。
% newgeometry=one|two|no
%   含义：规范中的自相矛盾之处，版芯是否包含页眉页脚，旧方法是按照包含页眉
%   页脚来设置。该选项是多选选项，如果设置为no，则版新为旧模板的版芯设置方法，
%   如果设置该选项one或two，分别对应两种页眉页码对应版芯线的相对位置。第一种
%   是严格按照规范要求，难看。第二种微调了页眉页码位置，好一点。默认two。
% debug=true|false
%   含义：是否显示版芯框和行号，用来调试。默认否。
% openright=true|false
%   含义：博士论文是否要求章节首页必须在奇数页，此选项不在规范要求中，按个
%   人喜好自行决定。 默认否。注意，窝工的默认情况是打印版博士论文要求右翻页
%   ，电子版要求非右翻页且无空白页。如果想DIY（或身不由己DIY）在什么地方右
%   翻页，将这个选项设置为false，然后在目标位置添加`\cleardoublepage`命令即
%   可。
% library=true|false
%   含义：是否为提交到图书馆的电子版。默认否。注意：如果设置成true，那么
%   openright选项将被强制转换为false。
% capcenterlast=true|false
%   含义：图题、表题最后一行是否居中对齐（我工规范要求居中，但不要求居中对
%   齐），此选项不在规范要求中，按个人喜好自行决定。默认否。
% subcapcenterlast=true|false
%   含义：子图图题最后一行是否居中对齐（我工规范要求居中，但不要求居中对齐
%   ），此选项不在规范要求中，按个人喜好自行决定。默认否。
% absupper=true|false
%   含义：中文目录中的英文摘要在中文目录中的大小写样式歧义，在规范中要求首
%   字母大写，在work样例中是全大写。该选项控制是否全大写。默认否。
% bsmainpagenumberline=true|false
%   含义：由于本科生论文官方模板的页码和页眉格式混乱，提供这个选项自定义设
%   置是否在正文中显示页码横线，默认否。
% bsfrontpagenumberline=true|false
%   含义：由于本科生论文官方模板的页码和页眉格式混乱，提供这个选项自定义设
%   置是否在前文中显示页码横线，默认否。
% bsheadrule=true|false
%   含义：由于本科生论文官方模板的页码和页眉格式混乱，提供这个选项自定义设
%   置是否显示页眉横线，默认显示。
% splitbibitem=true|false
%   含义：参考文献每一个条目内能不能断页，应广大刀客要求添加。默认否。
% newtxmath=true|false
%   含义：数学字体是否使用新罗马。默认是。
% chapterbold=true|false
%   含义：本科生章标题在目录和正文中是否加粗
% engtoc=true|false
%   含义：非博士生需要添加英文目录的，手动添加，如果是博士，此开关无效
% zijv=word|regu
%   含义：字距设置为规范规定33个字还是word中34个字。默认regu。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hithesis}

\graphicspath{{./figures/}}

\begin{document}
\frontmatter
\input{front/cover} % 封面
\makecover
\input{front/denotation}%物理量名称表,符合规范为主，有要求添加
\tableofcontents %目录
\mainmatter
%\include{body/xulun}













%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%绪论%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[绪论]{绪论}
\section{课题背景及研究的目的和意义}[Content specification]
要不看

\section{机器人视觉伺服系统概述}[Content specification]
分IBVS、PBVS和混合控制hvs。IBVS的基础原理从最原始的公式、解耦、图像矩到现在借助神经网络获直接端到端控制。绕不开当前特征与目标特征的提取的过程，但是方法越来越具有泛化性。
\section{基于视觉伺服的物体抓取发展现状}[Content specification]
抓取方法上：
经典的方法当然是人为的制作特征点或者标志，辅助视觉伺服。但是泛化性是不行的，需要避免人为的制作特征，让程序自主提取特征并生成目标。直接把整张图当作特征是后来的发展方向之一，这样必须要求目标与当前相似，伺服范围太小。神经网络兴起后，方法变得百花齐放。如现在不断发展的抓取合成（grasp systhesis），分经验法和深度法;神经网络自主生成目标图像但还是那个问题，这样必须要求目标与当前相似，伺服范围太小;手到眼系统，神经网络估计机械臂末端和目标的相对位姿，伺服后期会出现遮挡现象，一般还是要配合眼在手系统一起使用，涉及到多数据融合问题，非常复杂。通过渲染引擎获得目标图像，神经网络自动提取特征并匹配特征。大部分的方法无法回避需要线下制作目标图像的问题，对于一个完全未知的目标，如何在线上就能实时生成目标并执行抓取是对抓取未知目标任务的研究重点。
伺服性能上：
就算拥有一个合适的抓取方法，也需要有合适的控制律。低鲁棒性的伺服控制律，无法在应对各种位姿、形态的抓取对象中保证同样的抓取性能，这会大大降低最终的抓取成功率和伺服响应速度。为满足泛化性的需求，近几年有很多人投入到视觉伺服控制律的研究中，然后介绍各种控制方法。如何设计一个适合于当前抓取方法的控制律，是抓取未知目标任务的又一难点。


归功于近年深度网络的快速发展，抓取合成（grasp synthesis）的方法在这几年发展迅速。
\section{主要研究内容及章节安排}[Content specification]
介绍本文都要干些啥，IBVS相对于PBVS的优势，eye-to-hand系统
本文共分为六章，章节内容如下  
第一章：  
第二章：  
。。。





%%%%%%%%%%%%%%%%%%%%%%%%基于图像的视觉伺服理论研究%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[基于图像的视觉伺服（IBVS）理论研究]{基于图像的视觉伺服（IBVS）理论研究}
\section{引言}[Content specification]
绪论中介绍了IBVS相对于PBVS的优势：1.伺服精度不依赖于相机外参，深度鲁棒性强；2.直接得到指令，不需要轨迹规划，更易于实现实时闭环控制。因此认为IBVS更适合于未知位姿、形体的目标的抓取任务。IBVS作为本算法研究的基石之一，尤其需要十分严谨合理的模型建立、公式原理分析、完善的仿真系统和实物环境搭建，这会为之后的工作减少不少麻烦。本章中除了完成上述基础性工作，还在实物上成功运行了简单的基于特征点交互矩阵的IBVS。

\section{IBVS系统模型建立}[Content specification]
\subsection{系统坐标系建立}[Content specification]
对于IBVS系统，最需要关注的点有三个：机械臂末端、相机和目标。为了后续仿真程序实现和问题分析需要，建立系统的坐标系用于表述它们的位置。如图\ref{系统坐标系建立}所示：
\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter2/系统坐标系建立}
\caption{系统坐标系建立示意图}
\label{系统坐标系建立}
\end{figure}

该示意图中，$\lbrace$\textit{O}$\rbrace$、$\lbrace$\textit{E}$\rbrace$和$\lbrace$\textit{C}$\rbrace$分别表示物体坐标系、机器人末端坐标系，eye-to-hand系统中的相机坐标系。为了便于阐述坐标变换公式，用$\lbrace$\textit{B}$\rbrace$、$\lbrace$\textit{I}$\rbrace$和$\lbrace$\textit{CI}$\rbrace$表示机器人基坐标系、图像坐标系和像素坐标系，本研究中所说的基坐标系和世界坐标系是一个意思。将使用以下符号表示各个相对位姿变换：
$^{B}T_O$表示目标$\lbrace$\textit{O}$\rbrace$相对于基坐标系$\lbrace$\textit{B}$\rbrace$的坐标变换；
$^{B}T_E$表示机器人末端$\lbrace$\textit{E}$\rbrace$相对于基坐标系$\lbrace$\textit{B}$\rbrace$的变换。对机械臂末端使用的速度指令是在这个变化下进行的；
$^{C}T_O$表示目标$\lbrace$\textit{O}$\rbrace$相对于相机坐标系$\lbrace$\textit{C}$\rbrace$的坐标变换；
$^{E}T_C$表示相机$\lbrace$\textit{C}$\rbrace$相对于末端坐标系$\lbrace$\textit{E}$\rbrace$的坐标变换。一般情况下IBVS的伺服结果是相机正对目标，而真正抓取还是要依赖末端位置，所以这个变换是必要的；
$^{C}T_I$表示图像$\lbrace$\textit{I}$\rbrace$相对于相机坐标系$\lbrace$\textit{C}$\rbrace$的坐标变换。特征初始是在图像中获取的，需要这个变换使特征位置描述变成IBVS需要的形式\cite{zh1}。


\subsection{视觉模型建立}[Content specification]
IBVS不断地由特征偏差驱动着运行，而对特征的描述需要在$^{C}T_O$下进行。在图像中获取的特征需要经过图\ref{特征变换}所示的坐标系变换才能真正为IBVS所用：
\begin{figure}[h]
\centering
\includegraphics[width = 0.60\textwidth]{chapter2/特征变换}
\caption{特征变换过程图}
\label{特征变换}
\end{figure}


不例外地使用针孔模型描述从像素坐标系到机器人基坐标系中物体的映射,这张图引自这篇文献\cite{zh1}，这张针孔模型示意图十分典型。
\begin{figure}[h]
\centering
\includegraphics[width = 0.60\textwidth]{chapter2/视觉坐标系建立}
\caption{视觉坐标系建立示意图}
\label{视觉坐标系建立}
\end{figure}


图\ref{视觉坐标系建立}中用$X_CO_CY_C$描述$\lbrace$\textit{C}$\rbrace$，$xO_Iy$描述$\lbrace$\textit{I}$\rbrace$，P表示$\lbrace$\textit{C}$\rbrace$中的目标点，P'表示成像过程中投影到$\lbrace$\textit{I}$\rbrace$中的目标点。因为深度的存在，成像在二维像素坐标系中的图像所对应的目标可以是无穷多种情况，为了统一坐标变换形式，令目标深度$Z_C$为单位1，在相机坐标系$\lbrace$\textit{C}$\rbrace$和像素坐标系$\lbrace$\textit{CI}$\rbrace$中加上了一个过渡的图像坐标系$\lbrace$\textit{I}$\rbrace$。


相机内参由出厂地所给出，它包括相机的焦距$f$，相机放缩因子$f_x$和$f_y$，它们的单位为毫米；偏移量$c_x$和$c_y$，单位为像素，但是是浮点类型。由于Realsense D435i内置去畸变API，就不考虑畸变因素了。使用$\left[ u,v \right] ^T$表示像素坐标系下的目标点位置，$\left[ X_C,Y_C,Z_C \right] ^T$表示相机坐标系下的目标点位置，$Z_C$为相机深度，可以得到它们之间的关系：
\begin{equation}
	Z_C\left[ \begin{array}{c}
	\begin{array}{c}
	u\\
	v\\
\end{array}\\
	1\\
\end{array} \right] =\left[ \begin{matrix}
	f_x&		0&		c_x\\
	0&		f_y&		c_y\\
	0&		0&		1\\
\end{matrix} \right] \left[ \begin{array}{c}
	X_C\\
	Y_C\\
	Z_C\\
\end{array} \right] 
\label{像素到相机坐标变换} 
\end{equation} 


在已知相机内参的情况下，IBVS中所需要的特征位置描述就转换为找到特征对应的$\lbrace$\textit{CI}$\rbrace$中的位置$\left[ u,v \right] ^T$和深度相机测得对应点的深度$Z_C$。

\section{IBVS仿真系统实现}[Content specification]
\subsection{IBVS算法原理}[Content specification]
在本实验中所说的IBVS是基于特征点交互矩阵实现的\cite{chaumette2006visual}。它的基本思想是将特征点偏差通过交互矩阵（也叫图像雅可比矩阵）映射为末端速度指令。为了表述简洁，符号所代表的意思如下：$s^*=\left[ x^*,y^* \right] ^T$表示$\lbrace$\textit{C}$\rbrace$中对应的期望特征点坐标。$s=\left[ x,y \right] ^T$表示$\lbrace$\textit{C}$\rbrace$中对应的当前特征点坐标。$v_c=\left[ v_x,v_y,v_z,\omega _x,\omega _y,\omega _z \right] ^T$表示$\lbrace$\textit{B}$\rbrace$中相机的速度，其中相机包含质心线速度，和绕三个轴的角速度。由于相机和机械臂末端是固连且位置上接近的，所以它们的速度认为是一致的。使用交互矩阵$L_c$建立当前特征随时间变化率与相机位姿随时间变化率的关系：
\begin{equation}
\dot{s}=L_cv_c 
\label{当前特征变化率与相机位姿变化率的关系1} 
\end{equation} 


一般情况下，期望特征是不随时间改变的，或者变化甚微（在本研究中就是如此，所以进行近似），式\ref{当前特征变化率与相机位姿变化率的关系1}可以如方程组\ref{当前特征变化率与相机位姿变化率的关系2}中第一排表达式改写。另外，认为特征偏差随时间呈指数变化是合理的，因为它收敛快速且平滑\cite{chaumette2006visual}，于是可以得到方程组\ref{当前特征变化率与相机位姿变化率的关系2}：
\begin{equation}
\left\{ \begin{array}{c}
	\left( \dot{s}-\dot{s}^* \right) =\dot{e}=L_cv_c\\
	L_e=L_c\\
	\dot{e}=-\lambda e\\
\end{array} \right.  
\label{当前特征变化率与相机位姿变化率的关系2} 
\end{equation} 


其中$\dot{e}$为当前特征偏差随时间变化率，$\lambda$为比例系数，$\dot{e}$为当前特征偏差。通过对交互矩阵求广义逆，由方程组\ref{当前特征变化率与相机位姿变化率的关系2}可得到：
\begin{equation}
\left\{ \begin{array}{c}
	v_c=-\lambda L_{e}^{+}e\\
	L_{e}^{+}=\left( L_{e}^{T}L_e \right) ^{-1}L_{e}^{T}\\
\end{array} \right. 
\label{当前特征变化率与相机位姿变化率的关系3} 
\end{equation} 


其中$\lambda L_{e}^{+}$为交互矩阵广义逆。通过方程组\ref{当前特征变化率与相机位姿变化率的关系3}可以借助当前特征偏差求取机器人末端速度了。交互矩阵由特征点在图像中的位置及深度信息得到，每个点对应的交互矩阵如式\ref{交互矩阵}所示，若有多个点，公式中的交互矩阵就是每个点对应的交互矩阵在行方向的叠加。
 \begin{equation}
L_e=\left[ \begin{matrix}
	\frac{-1}{Z_C}&		0&		\frac{X_C}{Z_C}&		X_CY_C&		-\left( 1+X_{C}^{2} \right)&		Y_C\\
	0&		\frac{-1}{Z_C}&		\frac{Y_C}{Z_C}&		1+Y_{C}^{2}&		-X_CY_C&		-X_C\\
\end{matrix} \right] 
\label{交互矩阵} 
\end{equation} 

\subsection{机器人仿真模型搭建}[Content specification]
基于为整个系统搭建的坐标系和不同系的坐标转换关系，借助ROS的moveit工具（由于ROS2的moveit2尚未开发成熟，使用moveit代替），为敬科公司提供的JK机器人搭建仿真模型。moveit是一个开发的十分完善的工具包，不仅实现了机械结构的仿真，物理模型、碰撞体积和逆运动学都在包中相应地实现。本研究中，为了能更快地验证提出的算法，减少繁杂的处理，将把物体放到一个平整且颜色单一（在后续的研究中可以发现这些要求都不是必须的）的表面上。另外，相机一直保持俯视朝下，在X、Y轴方向的角度保持为0°，因此速度指令中$\omega _x$和$\omega _y$不论结果计算如何都给0。仿真效果图如图\ref{基于moveit机器人仿真模型实现}所示。
\begin{figure}[h]
\centering
\includegraphics[width = 0.55\textwidth]{chapter2/基于moveit机器人仿真模型实现}
\caption{基于moveit机器人仿真模型实现}
\label{基于moveit机器人仿真模型实现}
\end{figure}


图\ref{IBVS基础控制流程方框图}展现了整个系统最基础的控制方框图，在实验进行过程中会不断被改进，以应对实践中发生的各个问题。
\begin{figure}[h]
\centering
\includegraphics[width = 1.0\textwidth]{chapter2/IBVS基础控制流程方框图}
\caption{IBVS基础控制流程方框图}
\label{IBVS基础控制流程方框图}
\end{figure}


深度获取的方式是会根据深度条件切换的，如图\ref{深度传感器切换}所示。之所以加上这个切换，是因为Realsense D435i是基于结构光测量深度的，不可以测量过近距离的目标点。当相机在伺服末期十分靠近目标时，无法获得确切的目标点深度。所以在相机与目标点距离低于20cm时，会通过编码器读取末端下降的距离得到对应特征的深度，这在相机一直俯视向下时是可行的。


\begin{figure}[h]
\centering
\includegraphics[width = 0.60\textwidth]{chapter2/深度选择图}
\caption{深度传感器切换示意图}
\label{深度传感器切换}
\end{figure}


开启仿真节点后，可以在另一个节点中与该节点建立连接并发送速度指令，仿真节点会因此响应，并进行移动。moveit有自己的限位系统，在机器人进入奇异点或者超出移动范围时给予及时的警告，所以该机器人仿真模型多被用于对机器人是否进入奇异点的判断这样的定性分析，后文中真正的调参还是在实物上进行的。
\subsection{曲线绘制与相机轨迹记录}[Content specification]
曲线是分析问题非常重要的一环，所以仿真中应当有相应的曲线绘制。IBVS本质是将特征偏差作为控制器输入而映射成速度指令的控制系统，所以研究中最关心的是点在于特征偏差和速度指令，它们将被分别绘制到两张图中。曲线图中时间单位为秒。关于特征偏差图：系统中定义特征偏差是相机坐标系中被检测的特征点在X、Y方向的偏差，单位为米，该单位不被展现在曲线中，因为它的单位并不重要。关于末端速度指令图：为了与JK机器人需要的末端速度指令单位保持一致，所以线速度选取厘米每秒为单位，而角速度单位则为度每秒。


直观地展现相机的位移情况也是重要的，因为IBVS往往对机器人末端的运动轨迹十分不友好。如果当前的速度指令使机器人颤振，那么机器人已经进入了一个十分糟糕的姿态，通过分析相机的运动轨迹适当调节控制律参数也是非常好的解决方法。调用VISP库，对设定的参数进行视觉伺服仿真，实现的曲线绘制和相机轨迹绘制效果如图\ref{基于VISP视觉伺服仿真}所示。
\begin{figure}[h]
\centering
\includegraphics[width = 1.0\textwidth]{chapter2/基于VISP视觉伺服仿真}
\caption{基于VISP视觉伺服仿真}
\label{基于VISP视觉伺服仿真}
\end{figure}


提前在程序中设置好特征的位置，和与之对应的目标特征。将仿真得到的速度指令通过ROS2节点发布订阅机制传输给机器人仿真节点，机器人会相应地运动并使当前特征都到达目标特征处，从而到成到达目标位置处的目的。
\section{IBVS实际系统实现}[Content specification]
\subsection{IBVS实物系统平台搭建}[Content specification]
仿真终归只能用于定性分析。外界干扰、噪声多种多样，仿真中不可能把所有因素考虑进去。事实上，仿真跑出的结果往往十分顺滑，而实物中会反映很多处理不够细节的问题。我认为，IBVS算法在实物上成功运行，研究才算真正的开始。实物运行环境包括JK机器人和装载它并固定它底座的台子;用于承载目标物体的平台和目标;平台上铺盖的一层漫反射效果好且为单一白色的纸;机器人末端装配Realsense D435i深度相机（夹具暂时未装配，在正式夹取的时候会安装在末端）。之所以要铺一层纸，除了保证平面平整且颜色单一以外，还保证了深度相机不要因为丢失反射光导致获取无效数据。最终实物环境图如图\ref{实物环境搭建展示图}所示：
\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter2/实物环境搭建展示图}
\caption{实物环境搭建展示图}
\label{实物环境搭建展示图}
\end{figure}

\subsection{IBVS实际运行}[Content specification]
为了能更快地验证IBVS算法，在平台上贴了一张黑色方框码，经过视觉二值化、边缘获取、多边形拟合等处理。实验进行前会将机械臂末端调到目标位置，此时黑色方框会处于摄像头的中央，记录此时的四个点为目标特征。将机器人末端初始位置调至远离黑色方框的位置，距离目标位置的三维各个方向以及Z轴角度都有一定的偏差（$\lbrace$\textit{B}$\rbrace$中，$\varDelta X=0.3m,\varDelta Y=0.3m,\varDelta Z=0.5m$）。伺服过程中会不断捕获它的四个点作为特征，并计算特征偏差，最后映射成末端速度指令。伺服的成功证实了所实现的IBVS算法的正确性，同时也正式踏入对未知物体视觉伺服抓取的研究领域中。\ref{视觉伺服曲线绘制（初始）}展示了伺服过程中机械臂末端速度指令和各特征点的偏差对应的曲线。
\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{chapter2/视觉伺服曲线绘制（初始）}
\caption{视觉伺服曲线绘制（初始）}
\label{视觉伺服曲线绘制（初始）}
\end{figure}
\newpage
\section{本章小结}[Content specification]
本章讲述了基于特征点交互矩阵的IBVS的原理。在算法实现前建立系统坐标系和视觉坐标系，这会使之后的坐标描述便捷许多。搭建了IBVS的仿真运行环境，便于后续问题分析。搭建了实物运行环境，并成功运行了IBVS算法，这意味着研究真正的开始。
%%%%%%%%%%%%%%%%%%%%%%%%%%%抓取目标生成算法研究%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[伺服目标生成算法研究]{伺服目标自主生成算法研究}
\section{引言}[Content specification]
第二章完成了IBVS的模型建立、仿真系统搭建以及在实物上成功运行IBVS算法，这些为后续的研究提供了控制基础。现在机器人知道自己怎么动了，那么它面对未知物体抓取任务时如何知道自己往哪动呢？接下来，在本章中将对本次课题又一基石——伺服目标生成的研究进行展开。主要研究了基于模型的点云识别和抓取点生成网络两种方法，将它们进行比较后，选择了后者作为应对未知物体抓取任务时生成机器人伺服目标的方案。


\section{基于模型的点云识别与配准}[Content specification]
\subsection{方法陈述}
虽然物体样式多种多样，但它们总以一类一类的形式呈现，例如不同种类的苹果，形状会类似，苹果和球形状类似。对于每一类这样的物体称为一个模型，而对于每一个模型会有一个确定的抓取方式。先收集尽可能多的点云，计算它们的点云特征，对特征类似的进行聚类，计算每一类特征的平均值作为一个模型。每个模型会人为的设定点云中的部分特别位置的点作为期望特征。制作的其中一个圆柱类模型如图\ref{圆柱类点云模型}所示:
\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{chapter3/实时抓取点生成}
\caption{圆柱类点云模型}
\label{圆柱类点云模型}
\end{figure}


对于新来的未知物体，对每个模型进行ICP配准，解算当前物体相对模型的姿态，然后与原模型一致的方式提取当前点云特征，匹配模型的期望特征。将期望特征和提取的特征输入到IBVS控制器中完成伺服控制。


ICP即为迭代最近点法。假设有PA和PB两个点云，它们是相同物体的不同位姿时深度传感器获取的点云。该算法通过不断迭代它们之间的坐标变换矩阵参数的方式让两个点云尽可能的重叠\cite{chetverikov2002trimmed}。设PA和PB的点云分布分别为$p\left( x,y,z \right) $、$q\left( x,y,z \right) $，目前需要找到$P$到$Q$的旋转矩阵$R$和平移矩阵$T$，给出一个代价函数，通过最小二乘法求解最优解。在这之前，先表示出两点云的质心：
\begin{equation}
\left\{ \begin{array}{c}
	\vec{p}=\frac{1}{n}\sum_{i=1}^N{\vec{p}_i}\\
	\vec{q}=\frac{1}{n}\sum_{i=1}^N{\vec{q}_i}\\
\end{array} \right. 
\label{点云质心描述1} 
\end{equation}


然后从两个点集中的每个点减去相应的质心：
\begin{equation}
\left\{ \begin{array}{c}
	\vec{p}_t=\vec{p}_i-\vec{p}\\
	\vec{q}_t=\vec{q}_i-\vec{q}\\
\end{array} \right. 
\label{点云质心描述2} 
\end{equation}


则上述最优化目标函数可以转化为：
\begin{equation}
E=\sum_{i=1}^N{\left| \vec{q}_t-R\vec{p}_t \right|}
\label{ICP代价函数} 
\end{equation}


最优化问题最后分解为：
\begin{itemize}
\item[（1）]
求使代价函数$E$最小的旋转矩阵$R$。
\item[（2）]
求得平移矩阵$T=\vec{q}-R\vec{p}$.
\end{itemize}

\subsection{算法实现}
深度传感器获得的点云往往不全是目标物体的点云，在姿态匹配前，经过三个方向的点云截断滤波、去离群点、降采样操作，最后提取出目标点云。基于模板匹配点云识别过程中所提取的点云特征选取了VFH特征，它是一种全局特征，可以快速计算和匹配。算法的实现效果如图\ref{基于模型的点云识别实操图}和图\ref{基于模型的点云识别效果图}所示。


图\ref{基于模型的点云识别实操图}和图\ref{基于模型的点云识别效果图}分别是算法运行时第三方视角和电脑视角的图片，点云处理间隔为500ms，但识别和位姿解算间隔约为3s。
\newpage
\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter3/基于模型的点云识别实操图}
\caption{基于模型的点云识别实操图}
\label{基于模型的点云识别实操图}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter3/基于模型的点云识别效果图}
\caption{基于模型的点云识别效果图}
\label{基于模型的点云识别效果图}
\end{figure}
\subsection{方法小结}
该方法需要环境噪声较小时，才能正常匹配，消耗时间长，拖长了系统抓取物体所需运行的时间，且不可用于实时的目标特征生成，视觉伺服效果将会非常依赖初始的视觉、点云处理。另外，还存在模型制作困难、鲁棒性差等问题，这写让这个方法面对卷积神经网络完全没有一战之力。


\section{抓取点生成网络（GG-CNN）}[Content specification]
\subsection{方法陈述}
抓取点生成网络（GG-CNN）属于Grasp Synthesis中的一种，为PBVS量身订做的抓取点生成方案。Douglas Morrison等人在2018年提出了GG-CNN，该网络非常轻便快捷，可以通过输入深度图像，在19ms内输出图像中机器人的期望抓取位姿,最后依赖相机内参、外参计算出机械臂末端期望的位姿，通过PBVS来抓取未知物体\cite{morrison2018closing}。该工作最大的特点是，它能让视觉伺服实时生成期望位姿，伺服精度不再受初始计算的期望位姿影响。在后续研究中，会通过一个很特别而简单的方法将该网络所输出的结果应用于IBVS，但在这之前，先要弄清楚它的工作原理。


他们创新性在于提出了十分合适的网络输出。网络整体结构的设计是非常简单的，追求大感受野，然后就是很寻常的叠层。网络限定了机器人末端需要时二指的，视觉伺服控制中，相机必须保持时刻俯视，这也是在第二章实物搭建中这么做的原因之一。网络的本质是语义分割，输出4张与输入的300*300深度图像相同大小的图像$G=\left[ Q,W,\varPhi \left( \sin \theta ,\cos \theta \right) \right]$，其中使用$\sin \theta$和$\cos \theta$分别对应不同的输出图片。$Q$图像中每个像素代表这个点的抓取质量，它们都是被归一化的数据，1表示抓取质量很高，0表示这个点完全不值得抓取；$W$图像中每个像素代表抓取这个点所需要的二指张开宽度；$\varPhi$图像中每个像素代表抓取这个点所需要的二指沿Z轴旋转角度。


本文中认为他们所构建的网络有很大的优化空间。他们为了加快网络计算速度，所设计的层数太少，网络的非线性程度较低。所以使用1*1卷积层对网络非线性化。其次，作者非常喜欢使用大卷积核。实际上在机械臂末端运动过程中，物体在相机中的大小会有很大的改变，单一感受野并不能适应这样的变化，所以将大卷积核拆成不同尺度小卷积核的叠加。以上的优化并不会带来太多的计算量，因为改动的地方只是增加了1*1卷积层和拓展的小卷积核，它们本身不会带来什么计算量。最终网络如图\ref{GG-CNN改进}所示。
\begin{figure}[h]
\centering
\includegraphics[width = 1.0\textwidth]{chapter3/GG-CNN改进}
\caption{GG-CNN改进后网络架构}
\label{GG-CNN改进}
\end{figure}


\subsection{算法实现}
配好pytorch环境，使用Cornnel数据集。将计算出的最大抓取质量像素点作为长方形中心，$\varPhi$为长方形绕中心旋转角度，$W$作为长方形的长，长方形宽为它的1/2，绘制长方形框，将计算出的IoU（预测抓取框与标签抓取框对应交集与并集的比）作为预测准确率，选取数据的10\%作为测试集，跑通优化后的GG-CNN代码训练程序，约40 epoch时达到了对测试集的80\%准确率。这个准确率相对于原始程序上升了4\%，优化是有效的。对给定深度图，输出效果如图\ref{GG-CNN输出（初始）}所示：
\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter3/GG-CNN输出（初始）}
\caption{GG-CNN输出}
\label{GG-CNN输出（初始）}
\end{figure}


这里特别解释一下图\ref{GG-CNN输出（初始）}中图Q表示抓取质量热度图，颜色越偏暖色越值得被抓，图Angle表示抓取角度图，单位是弧度，可以看到对柱形物体，抓取角度在高抓取质量点处几乎一致。


在代码测试过程中发现Realsense D435i对深度的测量信噪比很低，测量深度波动很大，只依靠深度图像一阶图像矩无法稳定定位目标物体的位置，所以通过RGB图像和深度图像的一阶图像矩（如果能有更好的深度传感器，是不需要依赖颜色信息的）可以得到目标大致位置。使用这个作为中心对当前640*480的图像进行考虑边缘（如果超出原始图像范围，会平移中心）的300*300裁剪。将神经网络写成ROS2的一个节点，实时发布计算出的抓取点信息，视觉伺服节点会订阅它的主题，实时显示抓取方框。选择抓取对象为笔袋，图\ref{实时抓取点生成}为实时抓取点生成效果图。
\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter3/实时抓取点生成}
\caption{实时抓取点生成}
\label{实时抓取点生成}
\end{figure}


实时显示抓取框和之前只显示一次抓取框有很大的不同，因为神经网络输出的抓取质量在目标位置处会有很多相近的点，如果物体具有平移对称性或者旋转对称性，抓取点的位置会不断跳动，正如图\ref{神经网络原始输出}所示。这会使生成的目标特征不断摆动，导致系统失稳。（由于角度为弧度制过太小，输出时乘了100倍）
\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter3/神经网络原始输出}
\caption{神经网络原始输出}
\label{神经网络原始输出}
\end{figure}


在GG-CNN原文中给出的解决方式是计算抓取质量图中三个局部最大点，选择与上一次的抓取点最近的点作为本次的抓取点。但是研究中在复现他们的算法后依然无法解决波动很大的问题，甚至调高选择的局部最大点个数到10。本文认为这是物体平移、旋转对称性带来的必然结果，无论网络好坏，因为网络的输入是深度图像。所以该网络输出的抓取点位置信息无法使用，只能委曲求全使用RGB图像的一阶图像矩代替，毕竟在裁剪图像的时候就求取过一阶图像矩的值。颜色信息在所搭建的实验环境中是非常稳定的，这样稳定的抓取点中心可以被用于实时抓取点生成。只是使用该方法会带来另一个麻烦，在伺服末期，如果目标靠近平台边缘，摄像头将捕捉地面的颜色信息，会对抓取点中心的计算带来干扰，这个问题将在本文第四章和其它问题一并解决。


好在角度的波动能得到很好的解决。因为相机时刻俯视向下，所以相机期望的沿Z轴的旋转角度和抓取框旋转角度是一致的。对于这样一个线性系统，使用卡尔曼滤波可以有效抑制噪声。机械臂末端沿Z轴的角速度设为$\omega_z$，它由每次控制周期$\delta _t$的IBVS控制器计算出。机械臂末端旋转角度为$\theta_z$，可以通过读取机械臂编码器的值间接获取。可以写出状态、观测方程：
\begin{equation}
\hat{x}_{k+1}=\left[ \begin{array}{c}
	\theta _z\\
	\omega _z\\
\end{array} \right] _{k+1}=\left[ \begin{matrix}
	1&		0\\
	0&		\delta _t\\
\end{matrix} \right] \left[ \begin{array}{c}
	\theta _z\\
	\omega _z\\
\end{array} \right] _k=A\hat{x}_k
\label{状态方程}
\end{equation}
\begin{equation}
z_{k+1}=\theta _z=\left[ \begin{matrix}
	1&		0\\
\end{matrix} \right] \left[ \begin{array}{c}
	\theta _z\\
	\omega _z\\
\end{array} \right] _k=H\hat{x}_k
\label{观测方程}
\end{equation}


认为过程噪声$\omega_k$和观测噪声$\upsilon_k$都服从高斯分布：
\begin{equation}
\left\{ \begin{array}{c}
	p\left( \omega \right) \sim N\left( 0,Q \right)\\
	p\left( \upsilon \right) \sim N\left( 0,R \right)\\
\end{array} \right. 
\label{噪声分布}
\end{equation}


则有协方差矩阵的预测方程：
\begin{equation}
\hat{P}_{k+1}=AP_kA^T+Q
\label{协方差预测}
\end{equation}


设卡尔曼滤波的增益为$K$，则根据以上条件求得：
\begin{equation}
K_{k+1}=\hat{P}_{k+1}H^T\left( H\hat{P}_{k+1}H^T+R \right) ^{-1}
\label{卡尔曼滤波的增益}
\end{equation}


使用观测器数据对当前预测状态进行更新，对卡尔曼滤波器的初始化中，令$R$、$Q$都为单位矩阵：
\begin{equation}
x_{k+1}=\hat{x}_{k+1}+K_{k+1}\left( z_{k+1}-H\hat{x}_{k+1} \right) 
\label{卡尔曼滤波的增益}
\end{equation}


再使用新的预测的状态更新协方差矩阵：
\begin{equation}
P_{k+1}=\left( I-K_{k+1}H \right) \hat{P}_{k+1}
\label{更新协方差矩阵}
\end{equation}


在机械臂静止不动时，神经网络预测的抓取点中心、角度随时间变化曲线如图\ref{抓取点中心方法替代与角度滤波}所示。得到的抓取中心只有1到2像素点的波动，预测的抓取角度几乎不再波动。
\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter3/抓取点中心方法替代与角度滤波}
\caption{抓取点中心方法替代与角度滤波}
\label{抓取点中心方法替代与角度滤波}
\end{figure}

\subsection{方法小结}
GG-CNN是十分契合本次研究目的的研究成果，因此它被选作未知物体抓取算法中解决机械臂怎么去抓取目标的方法。但是在复现工作中遇到了抓取中心波动过大的无法解决的问题，只能使用RGB图像的一阶图像矩方法代替它进行抓取点中心生成，最终只保留了该网络输出的角度项，在经过卡尔曼滤波处理后拥有比较稳定的输出值。

\section{本章小结}[Content specification]
本章研究了两种伺服目标自主生成的算法，比较后，选择了后者作为后续研究生成伺服目标的主要方法。相比于传统的基于模型获取抓取点的方法，神经网络有更好的鲁棒性、实时性。但是目前选取的GG-CNN并不是完美的，在实时伺服中它的抓取点中心因为目标物体平移、旋转对称性的存在，波动大到无法使用滤波的方法来抑制了。所以使用RGB一阶图像矩来生成抓取点中心。对于网络输出的抓取角度使用线性卡尔曼滤波对波动进行有效的抑制。


%%%%%%%%%%%%%%%%%%基于GG-CNN和IBVS的未知物体抓取算法实现%%%%%%%%%%%%%%%%%%%
\chapter[未知物体抓取算法实现]{未知物体抓取算法实现}
\section{引言}[Content specification]


\section{机器人视觉伺服系统概述}[Content specification]



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%IBVS控制律优化%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[IBVS控制律优化]{IBVS控制律优化}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%实验设计与验证%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{实验设计与验证}
\section{引言}[Content specification]
前面的章节中详细说明了通过视觉伺服抓取未知物体的思路、视觉伺服控制律的优化以及它们的实现过程。在前人的研究成果基础上，这套未知物体抓取算法提出创新性的特征点提取、匹配方式和与之配套的视觉伺服控制律。它将在更少的环境条件限制下拥有更高的未知物体抓取率和更理想平滑的曲线。本章将对一共八个家常物体，每个物体八种位姿进行抓取实验，然后将抓取成功率与各生成抓取合成类方法进行比较，证明这套未知物体抓取算法的优越性。另外，会对不同物体同一位姿、相同物体不同位姿的伺服曲线进行记录，通过超调、响应时间等参数反映算法应对多种多样情况下的高鲁棒性。

\section{实验设计}[Content specification]
对于未知物体抓取任务，抓取成功率是最需要关注的指标。为了使成功率的测试结果能体现系统应对各种情形下的抓取能力，需要实验中的抓取目标种类多样化和位姿多样化。另外，为了能与提出GG-CNN原文献抓取成功率结果相对公平性的横向比较，尽量搭建相同的实验环境和使用相同的抓取目标。


根据算法的实现过程，实验过程中存在的限制条件如下：
\begin{itemize}
\item[（1）]
由于研究中GG-CNN对输入深度图像的尺寸需求和清晰度需求，需要对目标的进行范围裁剪，这时会使用RGB信息确定目标的中心大致范围，所以实验中在平台上铺盖的白纸是必须的。如果有更好的网络能不需要裁剪并稳定指出目标位置，实验环境中白色背景的条件可以被去除。
\item[（2）]
GG-CNN的输出形式决定了如果要使用它的输出结果作为视觉伺服的依据，必须保证相机时刻俯视0°朝下，所以对末端的沿X、Y轴角速度指令不论计算结果如何都会在给机械臂下达命令那一步置零。
\item[（3）]
深度传感器使用的Realsense D435i，它是基于结构光的原理测量深度的，所以对漫反射能力差的目标，获取的深度值几乎是无效值，所以选用的目标都是漫反射效果好的物体。另外，它的深度测量精度为1mm，这决定了选用目标不可以是铅笔等过小的物体，当然，也不可以过大，这会让神经网络无法区分谁是目标谁是背景。
\end{itemize}


实物环境搭建如图\ref{实物环境搭建展示图}，这与GG-CNN原文是类似的。但值得一提的是，经过第四章中对RGB图像和深度图像的特别处理，本文中机械臂的运动范围可以超出平台范围（但初始情况时，目标必须在相机视野范围内），这是相对GG-CNN原文具有优势的地方。根据限制条件（3）和仿照GG-CNN原文使用的抓取目标，选择了八种家常抓取目标用于本次实验的各种测试与验证，它们是什么{\color{red}正如图所示}：


抓取目标将以对称的姿态在平台的八个方位的位置摆放，这么做可以很全面地测试伺服系统应对多样化的目标位姿的伺服情况，摆放示意图{\color{red}如图？所示}：


{\color{red}六轴JK机械臂初始状态各编码器显示数值为：机械臂末端在$\lbrace$\textit{B}$\rbrace$中的初始状态为：。在这样的初始参数设计下，摄像头获取的图片如图？所示，正好可以将整个平台尽收眼底。
相对机械臂末端，目标摆放位置和对应姿态如表？所示}


实验中将会在八种不同的物体的八种不同位姿情况下对目标进行抓取，记录的实验结果包括是否成功抓取，伺服超调量和响应时间。响应时间不包括抓物体所需时间，因为抓取是开环控制的，所需时间固定，没有记录的必要。开环抓取效果在第四章有详细介绍。然后会与各个基于生成抓取合成方法来抓取未知物体的文献成果进行比较。最后对不同物体相同位姿以及抓取率百分之百的一个物体不同位姿伺服曲线进行分析，并与同样基于GG-CNN但使用PBVS+IBVS来抓取未知物体的文献成果\cite{haviland2020control}进行比较。
\section{人机交互界面设计}[Content specification]


\section{实验验证}[Content specification]
\subsection{抓取成功率测试}[Content specification]
对总共64种情况的实验结果进行记录，不仅记录总的抓取成功率，同时记录相同物体的抓取成功率和相同位姿对应不同物体的抓取成功率，绘制表格如下：


通过该表格展现的数据，反应所提出的未知物体抓取算法特点如下：


将实验获得的数据与各类成抓取合成方法的文献成果在一张表格中进行展现：


表格反映了。。。
\subsection{抓取性能验证}[Content specification]
选择的目标为。。。，把它不同的摆放位姿情况下的特征偏差曲线绘制到同一坐标系下，将不同情况对应的超调量和响应时间绘制到直方图中，如图？所示：


从图？中可以看出。。。


选择的位姿为。。。，把对应的不同目标情况下的特征偏差曲线绘制到同一坐标系下，将不同情况对应的超调量和响应时间绘制到直方图中，如图？所示：


图？为同样基于GG-CNN但使用PBVS+IBVS来抓取未知物体的伺服过程中特征偏差曲线，他还测量了运动物体的伺服情况，相关曲线忽略。比较静止物体的伺服情况，有以下结论。。。
\section{本章小结}[Content specification]
本章在八种不同家常的物体的八种不同位姿情况下对目标进行抓取实验，在所提出的特征提取、匹配的方法和与之配套的伺服控制律的加持下，在环境限制条件更少的情况下，拥有比许多类似工作的研究成果更高的抓取成功率。与另一个基于GG-CNN抓取未知目标文献成果比较，拥有更平滑而理想的伺服曲线。通过以上实验证明了本研究所提出的未知物体抓取算法有高鲁棒性和高伺服性能。
%\include{body/introduction}

\backmatter
\include{back/conclusion}   % 结论
\bibliographystyle{hithesis} %如果没有参考文献时候
\bibliography{reference}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%-- 注意：以下本硕博、博后书序不一致 --%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% 硕博书序
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%\begin{appendix}%附录
%\input{back/appA.tex}
%\end{appendix}
%\include{back/publications}    % 所发文章
%\include{back/ceindex}    % 索引, 根据自己的情况添加或者不添加，选择自动添加或者手工添加。
%\authorization %授权
%%\authorization[scan.pdf] %添加扫描页的命令，与上互斥
%\include{back/acknowledgements} %致谢
%\include{back/resume}          % 博士学位论文有个人简介
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% 本科书序为：
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
 \authorization %授权
 % \authorization[scan.pdf] %添加扫描页的命令，与上互斥
 \include{back/acknowledgements} %致谢
% \begin{appendix}%附录
% \input{back/appendix01}%本科生翻译论文
% \end{appendix}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% 博后书序
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% \include{back/acknowledgements} %致谢
% \include{back/doctorpublications}    % 所发文章
% \include{back/publications}    % 所发文章
% \include{back/resume}          % 博士学位论文有个人简介
% \include{back/correspondingaddr} %通信地址
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\end{document}
% Local Variables:
% TeX-engine: xetex
% End:
