% !Mode:: "TeX:UTF-8"
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%          ,
%      /\^/`\
%     | \/   |                CONGRATULATIONS!
%     | |    |             SPRING IS IN THE AIR!
%     \ \    /                                                _ _
%      '\\//'                                               _{ ' }_
%        ||                     hithesis v3                { `.!.` }
%        ||                                                ',_/Y\_,'
%        ||  ,                   dustincys                   {_,_}
%    |\  ||  |\          Email: yanshuoc@gmail.com             |
%    | | ||  | |            https://yanshuo.name             (\|  /)
%    | | || / /                                               \| //
%    \ \||/ /       https://github.com/dustincys/hithesis      |//
%      `\\//`   \\   \./    \\ /     //    \\./   \\   //   \\ |/ /
%     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[fontset=fandol,type=bachelor,campus=harbin,bsmainpagenumberline=true]{hithesisbook}
% 此处选项中不要有空格
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 必填选项
% type=doctor|master|bachelor|postdoc
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 选填选项（选填选项的缺省值已经尽可能满足了大多数需求，除非明确知道自己有什么
% 需求）
% campus=shenzhen|weihai|harbin
%   含义：校区选项，默认harbin
% glue=true|false
%   含义：由于我工规范中要求字体行距在一个闭区间内，这个选项为true表示tex自
%   动选择，为false表示区间内一个最接近版心要求行数的要求的默认值，缺省值为
%   false。
% tocfour=true|false
%   含义：是否添加第四级目录，只对本科文科个别要求四级目录有效，缺省值为
%   false
% fontset=windows|mac|ubuntu|fandol|adobe
%   含义：设置字体，默认情况会自动识别系统，然后设置字体。后两个是开源字体，自行
%   下载安装后设置使用。windows是中易字库，窝工默认常用字体，绝对没毛病。mac和
%   ubuntu 默认分别是华文和思源字库，理论上用什么字库都行。后两种开源字库的安装
%   方法到谷歌上百度一下什么都有了。Linux非ubuntu发行版、非x86架构机器等如何运行
%   可到github issue上讨论。
% tocblank=true|false
%   含义：目录中第一章之前，是否加一行空白。缺省值为true。
% chapterhang=true|false
%   含义：目录的章标题是否悬挂居中，规范中要求章标题少于15字，所以这个选项
%   有无没什么用，除了特殊需求。缺省值为true。
% fulltime=true|false
%   含义：是否全日制，缺省值为true。非全日制如同等学力等，要在cover中设置类
%   型，封面中不同格式
% subtitle=true|false
%   含义：论文题目是否含有副标题，缺省值为false，如果有要在cover中设置副标
%   题内容，封面中显示。
% newgeometry=one|two|no
%   含义：规范中的自相矛盾之处，版芯是否包含页眉页脚，旧方法是按照包含页眉
%   页脚来设置。该选项是多选选项，如果设置为no，则版新为旧模板的版芯设置方法，
%   如果设置该选项one或two，分别对应两种页眉页码对应版芯线的相对位置。第一种
%   是严格按照规范要求，难看。第二种微调了页眉页码位置，好一点。默认two。
% debug=true|false
%   含义：是否显示版芯框和行号，用来调试。默认否。
% openright=true|false
%   含义：博士论文是否要求章节首页必须在奇数页，此选项不在规范要求中，按个
%   人喜好自行决定。 默认否。注意，窝工的默认情况是打印版博士论文要求右翻页
%   ，电子版要求非右翻页且无空白页。如果想DIY（或身不由己DIY）在什么地方右
%   翻页，将这个选项设置为false，然后在目标位置添加`\cleardoublepage`命令即
%   可。
% library=true|false
%   含义：是否为提交到图书馆的电子版。默认否。注意：如果设置成true，那么
%   openright选项将被强制转换为false。
% capcenterlast=true|false
%   含义：图题、表题最后一行是否居中对齐（我工规范要求居中，但不要求居中对
%   齐），此选项不在规范要求中，按个人喜好自行决定。默认否。
% subcapcenterlast=true|false
%   含义：子图图题最后一行是否居中对齐（我工规范要求居中，但不要求居中对齐
%   ），此选项不在规范要求中，按个人喜好自行决定。默认否。
% absupper=true|false
%   含义：中文目录中的英文摘要在中文目录中的大小写样式歧义，在规范中要求首
%   字母大写，在work样例中是全大写。该选项控制是否全大写。默认否。
% bsmainpagenumberline=true|false
%   含义：由于本科生论文官方模板的页码和页眉格式混乱，提供这个选项自定义设
%   置是否在正文中显示页码横线，默认否。
% bsfrontpagenumberline=true|false
%   含义：由于本科生论文官方模板的页码和页眉格式混乱，提供这个选项自定义设
%   置是否在前文中显示页码横线，默认否。
% bsheadrule=true|false
%   含义：由于本科生论文官方模板的页码和页眉格式混乱，提供这个选项自定义设
%   置是否显示页眉横线，默认显示。
% splitbibitem=true|false
%   含义：参考文献每一个条目内能不能断页，应广大刀客要求添加。默认否。
% newtxmath=true|false
%   含义：数学字体是否使用新罗马。默认是。
% chapterbold=true|false
%   含义：本科生章标题在目录和正文中是否加粗
% engtoc=true|false
%   含义：非博士生需要添加英文目录的，手动添加，如果是博士，此开关无效
% zijv=word|regu
%   含义：字距设置为规范规定33个字还是word中34个字。默认regu。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hithesis}
\usepackage{gensymb}
%\usepackage{enumerate}
\graphicspath{{./figures/}}

\begin{document}
\frontmatter
\input{front/cover} % 封面
\makecover
%\input{front/denotation}%物理量名称表,符合规范为主，有要求添加
\tableofcontents %目录
\mainmatter
%\include{body/xulun}













%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%绪论%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[绪论]{绪论}
\section{课题背景及研究的目的和意义}[Content specification]
随着工业、服务业智能化发展，机器人需要在更苛刻的环境下自主地完成更严格要求的操作。视觉伺服的不接触性让智能抓取技术变得灵活可靠，硬件的制造工艺快速进步使其稳定性、实时性得到很大程度的增长。而相比传统机器人，采用视觉伺服的方式不仅能让处理机器人控制问题时拥有更充分直接的信息，而且其非接触式的特点让机器人能更灵活地工作\cite{徐鑫莉2015工业机器人视觉伺服控制系统设计,陶波2016机器人无标定视觉伺服控制研究进展}。


通常情况下，我们使用教学的方式让机械臂记录如何去完成自己的抓取动作，因为给予它们的任务往往是简单而重复的。然而在出现抓取目标多样化，甚至只是出现位姿的改变，都需要重新进行教学\cite{tsuchida2021characteristics}。另外，这样的系统对机器人驱动误差或传感器误差没有任何鲁棒性，机器人损坏后更换完各个设备，又将面临重新调参，这完全是可以避免的。人工控制更为灵活，但人需要长时间的培养和经验积累才能对机器较为精准快速地控制，并且直接控制的精准和稳定性已经无法满足现代工厂的需求了。机器人需要能自主根据环境进行决策，自主可靠的解决方案中很难不考虑视觉伺服，视觉伺服几乎已是现在自动化工厂不可缺少的部分。由于产品寿命短，生产线经常变动，工业机器人不可避免的要从固定模式的运动控制到更为灵活的行为模式中去。它能让机器人具有“眼睛”这样一个强力的感官，视觉信息更能适应于大部分场景，以应对不同的任务需求。这么做不但减少人力，节约了劳动力资源，而且产品的质量、生产效率都有很大程度的提升\cite{zh1}。但当今视觉伺服在工业生产的应用中仍存在自适应能力差、精度低、稳定性不够等技术问题。
\begin{figure}[htbp]
\centering
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[scale=1.0]{chapter1/工业机器人}
\caption{工业机器人与视觉伺服}
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[scale=1.0]{chapter1/医用机器人}
\caption{医用机器人与视觉伺服}
\end{minipage}
\end{figure}


智能服务型机器人是当今另一个机器人领域中很火热的方向，医疗服务是服务型机器人一大应用。外科手术机器人在多年前已被广泛投入
使用，医生可以通过摄像头采集到的视觉信息，自身在另一端遥控患者这一端的机械臂操纵手术用具对患者完成手术，这不但确保了手术的精细程度，而且降低了患者手术风险及感染机率\cite{机器人视觉伺服研究进展}。
但这样的技术还是开环控制，更多的依然依赖大夫本身通过视觉图像的判断，人为主动因素仍较高\cite{外文1,外文2}。对于现在的微创手术，精细程度要求更高
需要机器人更主动精准的定位和控制。而视觉伺服引入医疗服务，机器人能自动通过图像处理得到患者病灶区精准位置，再通过手术导航软件的引导，手术机械能自动运动到病灶区域，补偿人为控制的误差进行手术\cite{卢钰2016基于双目磁锚定手术机器人无标定视觉伺服控制研究}，这种技术能更进一步提高手术精度。如果视觉伺服的自适应定位性能和实时性不够，这些好的思想无法应用于实际，医疗机器人也难以在这个领域发展。


综上所述，研究自主可靠的机器人视觉伺服系统至关重要。本文将针对未知物体抓取算法展开研究，这包括能够在非结构化环境中运行，同时抓住以前从未见过的物品。对于一个更自动化的机器人视觉伺服系统，它持有更少的先验信息（未知物体、随意位姿）但也能很好的完成抓取任务。这是在机械臂视觉伺服抓取领域中大家都在攻克的难关。因为这项功能的完美实现可以大幅度降低工业生产、物流运输等机械臂的调试、维修成本，减少人力操作和节约劳动力资源。所以对机械臂抓取未知物体算法的研究十分具有意义。


\section{机器人视觉伺服发展现状}[Content specification]
常见的视觉伺服分基于图像的视觉伺服（IBVS）、基于位置的视觉伺服（PBVS）和混合视觉伺服（HVS）。IBVS将图像中的特征相对期望特征的偏差作为控制系统的输入控制执行器速度直至相机中的特征到达期望特征为止。该方法对深度测量具有很好的鲁棒性，但精度依赖于图像信噪比且是一种局部收敛的控制。如果这个偏差过大，视觉伺服系统将会发散。PVBS通过图像中的目标位置计算出世界坐标系中目标的位姿，将当前执行器位姿和目标位姿做差输入控制器中。该方法具有全局收敛的特点，但十分依赖内外参、深度测量的精度，同时不可避免的需要轨迹规划。HVS结合了上述两种方法的特点，但算法实现复杂、可调节参数较多。


物体抓取任务一般出现在工业生产和物流运输中，运作具有局限性，抓取控制中全局收敛是不必要的。另外，这样的抓取环境往往多样而复杂，使用PBVS面对不同的抓取场景都要标定外参，这项工作是繁琐且多余的。所以本文集中研究基于IBVS的未知物体抓取算法，并主要对IBVS的发展现状进行陈述。


F Chaumette, S Hutchinson对IBVS的最基础的原理进行了详细说明\cite{chaumette2006visual}。该方法使用的特征是图像中提取的特征点，同时也对应到现实世界中，利用它们的信息计算交互矩阵，从而计算机械臂末端速度指令。所以为了保证特征提取与匹配的稳定性，常常使用人工制作的特征。它们的另一篇文章讲述了期望、当前交互矩阵的平均值作为新的交互矩阵有更好的伺服效果\cite{chaumette2007visual}。图\ref{IBVS基础复现}是在复现他们成果时的截图。
\begin{figure}[htbp]
\centering
\includegraphics[scale=0.5]{chapter1/IBVS基础成果复现}
\caption{IBVS基础成果复现}
\label{IBVS基础复现}
\end{figure}


图像矩早在1960年被提出并广泛应用于模式识别，它是一种全局特征，旨在克服传统特征提取受噪声影响较大的困难。F. Chaumette将图像矩用于IBVS中，并推导出了基于图像矩的交互矩阵\cite{chaumette2004image},它的想法在对平面物体和对称物体的试验中得到很好的实现。传统特征点检测器对特征点检测的不稳定问题可以通过使用图像矩解决，基于图像矩的IBVS在2000到2010年有很多人投入研究，有许多不错的研究成果。


C Liu, R Chen等人提出了基于集空间的IBVS\cite{liu2017set}。这是十分新颖的方法，不需要用于图像特征的提取、匹配和跟踪的复杂的图像处理技术。相反，它只需要一个简单的匹配算法并在集合空间中构建视觉偏差。每个错误主要与一个相机的自由度有关，它们设计了一个解耦控制律解决了这个问题。该方法鲁棒性良好，不需要用到相机内参。


随着神经网络的再兴起，在IBVS中也卷起了神经网络热潮。F Tokuda, S Arai等人提出一种端到端的神经网络\cite{tokuda2021convolutional}，自主提取特征并与目标图像的特征匹配，直接获取末端速度指令。这是一种几乎完全依赖数据的简洁明了的方式。


综合以上提到的实现IBVS的思路，科研工作者几乎都在寻找一种新的特征提取方式来解决传统方法实时性差、兴趣点获取不稳定、鲁棒性差等问题。方法的发展也逐渐变得自适应，应用范围变得不再局限。
\section{基于视觉伺服的物体抓取}[Content specification]
基于视觉伺服的物体抓取的核心集中于两个问题：抓取方法和伺服性能。抓取方法决定了算法的上限，优秀的抓取方法能让系统拥有不俗的自适应能力；而伺服性能决定了算法的下限，它往往需要配合抓取方法进行设计。
\subsection{抓取方法}
经典的方法当然是人为的制作特征点或者标志，辅助视觉伺服。但是这种做法下泛化性是不行的，需要避免人为的制作特征，让程序自主提取特征并生成目标。直接把整张图当作特征是后来的发展方向之一，这样必须要求目标与当前相似，伺服范围太小。所以为了解决这些问题，多种多样的方法也应运而生。
\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{chapter1/GG-CNN实物伺服效果}
\caption{GG-CNN实物伺服效果}
\label{GG-CNN实物伺服效果}
\end{figure}


发展逐渐成熟的抓取合成技术（grasp systhesis），通过抓取图像的图像特征分析出抓取该目标时机械臂应处于的位姿，以此来规划机械臂的伺服过程\cite{varley2015generating}。该方法分经验法和深度法，随着深度网络的发展，深度法已经成为抓取合成技术的主要趋势，图\ref{GG-CNN实物伺服效果}是抓取合成技术一个典型方法GG-CNN的实物伺服过程图。通过神经网络自主生成目标图像是前几年许多研究者投入的领域之一，但还是相同的问题，这样必须要求目标与当前相似，没有解决伺服范围太小的问题。通过渲染引擎获得目标图像，神经网络自动提取特征并匹配特征\cite{adrian2022dfbvs}，该方法从一定程度上解决了目标需要与当前相似的问题，但是具有伺服指令不够平滑的缺陷。针对眼到手（eye-to-hand）系统的神经网络，神经网络估计机械臂末端和目标的相对位姿，伺服后期会出现遮挡现象，一般还是要配合眼在手系统一起使用，涉及到多数据融合问题，较为复杂\cite{rauch2019learning}，图\ref{语义分割IBVS}为他们是语义分割的成果图。大部分的方法无法回避需要线下制作目标图像的问题，对于一个完全未知的目标，如何在线上就能实时生成目标并执行抓取是对抓取未知目标任务的研究重点。
\begin{figure}[h]
\centering
\includegraphics[width = 0.6\textwidth]{chapter1/语义分割IBVS}
\caption{语义分割与IBVS}
\label{语义分割IBVS}
\end{figure}
\subsection{伺服性能}
尽管拥有一个不错的抓取方法，但也需要有合适的控制律。低鲁棒性的伺服控制律，无法在应对各种位姿、形态的抓取对象中保证同样的抓取性能，这会大大降低伺服性能和最终的抓取成功率，导致抓取方法不具有实用性。对于大多数研究成果中采用的抓取方法是全开环的，而并非视觉伺服，生成一次抓取点就会执行抓取\cite{lenz2015deep,pinto2016supersizing,johns2016deep}。这样伺服精度太依赖于第一次计算，抓取成功率不会很高，算法泛用性也不会很高。而对于一些能实时生成抓取点的成果，对视觉伺服控制器的设计是粗糙的\cite{haviland2020control,viereck2017learning}。他们甚至尝试抓取动态的未知物体，这样他们算法的效果不能被尽可能的发挥。为满足泛化性的需求，如何结合当前伺服特性设计一个适合于当前抓取方法的控制律，是抓取未知目标任务的又一大课题。

\newpage
\section{主要研究内容及章节安排}[Content specification]
本文以手在眼（eye-in-hand）机械臂视觉伺服系统为研究对象，以抓取未知形状随机位姿的物体为目标展开研究，旨在面对多样的抓取对象，与其它相关工作成果比较能拥有更高的抓取成功率并且拥有很好的伺服性能。为此，首先搭建IBVS系统模型，在仿真和实物上实现IBVS算法。然后研究能自主生成抓取期望的算法，选择合适的方法并优化它。对于抓取点生成网络在IBVS中的使用，保证特征提取、匹配的可靠性和准确性。随后在当前算法基础上给出适合它的IBVS控制律。最后通过实物实验验证提出的基于抓取点生成网络和IBVS的未知物体抓取算法的高抓取率、高伺服性能。

本文共分为六章，章节内容如下：

 
第一章：介绍了课题背景及研究的目的和意义，陈述了机器人视觉伺服的发展历程和现状。然后讲述了实现基于视觉伺服的物体抓取的各种方法，比较他们的优劣，最终确定自己在这个问题上的的研究、实施方案并按章节列写出。 

 
第二章：针对未知物体抓取任务搭建IBVS系统模型，其中包括系统坐标系和视觉坐标系的建立。针对以上建立的模型和基于特征点交互矩阵的IBVS原理，实现IBVS系统仿真，根据预设的目标得到正确的末端速度指令曲线和相机轨迹来证明算法的正确性。最后搭建实物平台，在实物上成功运行IBVS算法。


第三章：研究了基于模型的点云识别与配准和抓取点生成网络（GG-CNN）两种生成伺服目标的方法，通过比较它们的优劣选择了后者作为伺服目标生成方案。最后采取滤波等方式抑制了神经网络的输出波动。


第四章：通过图像处理解决各种环境干扰问题。将抓取点生成网络运用到IBVS中，结合ORB特征的提取与匹配，提出了一种基于抓取点的IBVS，根据不断失败的经验，又提出了基于抓取点的自定义特征。该方法解决了速度指令波动大和关于特征点匹配的可靠性差、顺序不确定等问题。最后优化了自定义特征分布，增强了特征的可靠性。


第五章：针对当前伺服性能差的问题，通过PD控制器和自适应算法优化了系统伺服性能。设计滑模控制器抑制了物体表面深度估计不确定性带来的系统模型摄动的问题所造成的指令波动。注意到出现的特征偏差收敛速度不一致问题，提出了一种自适应的算法，再次提升了系统的伺服性能。


第六章：最后，设计实验，通过多样的物体、多样的位姿的抓取实验验证提出的未知物体抓取算法的有效性。与相关工作进行比较，证明当前算法的高自适应能力和高伺服性能。


为了让章节间的关系更清晰地展现，将各章节之间的关系绘制如框图\ref{章节流程图}所示：
\begin{figure}[h]
\centering
\includegraphics[scale=1.0]{chapter1/章节流程图}
\caption{各章节关系图}
\label{章节流程图}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%基于图像的视觉伺服理论研究%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[基于图像的视觉伺服（IBVS）理论研究]{基于图像的视觉伺服（IBVS）理论研究}
\section{引言}[Content specification]
绪论中介绍了IBVS相对于PBVS的优势：1.伺服精度不依赖于相机外参，深度鲁棒性强；2.直接得到指令，不需要轨迹规划，更易于实现实时闭环控制。因此认为IBVS更适合于未知位姿、形体的目标的抓取任务。IBVS作为本算法研究的基石之一，尤其需要十分严谨合理的模型建立、公式原理分析、完善的仿真系统和实物环境搭建，这会为之后的工作减少不少麻烦。本章中除了完成上述基础性工作，还在实物上成功运行了简单的基于特征点交互矩阵的IBVS。

\section{IBVS系统模型建立}[Content specification]
\subsection{系统坐标系建立}[Content specification]
对于IBVS系统，最需要关注的点有三个：机械臂末端、相机和目标。为了后续仿真程序实现和问题分析需要，建立系统的坐标系用于表述它们的位置。如图\ref{系统坐标系建立}所示：
\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter2/系统坐标系建立}
\caption{系统坐标系建立示意图}
\label{系统坐标系建立}
\end{figure}

该示意图中，$\lbrace$\textit{O}$\rbrace$、$\lbrace$\textit{E}$\rbrace$和$\lbrace$\textit{C}$\rbrace$分别表示物体坐标系、机器人末端坐标系，eye-to-hand系统中的相机坐标系。为了便于阐述坐标变换公式，用$\lbrace$\textit{B}$\rbrace$、$\lbrace$\textit{I}$\rbrace$和$\lbrace$\textit{CI}$\rbrace$表示机器人基坐标系、图像坐标系和像素坐标系，本研究中所说的基坐标系和世界坐标系是一个意思。将使用以下符号表示各个相对位姿变换：
$^{B}T_O$表示目标$\lbrace$\textit{O}$\rbrace$相对于基坐标系$\lbrace$\textit{B}$\rbrace$的坐标变换；
$^{B}T_E$表示机器人末端$\lbrace$\textit{E}$\rbrace$相对于基坐标系$\lbrace$\textit{B}$\rbrace$的变换。对机械臂末端使用的速度指令是在这个变化下进行的；
$^{C}T_O$表示目标$\lbrace$\textit{O}$\rbrace$相对于相机坐标系$\lbrace$\textit{C}$\rbrace$的坐标变换；
$^{E}T_C$表示相机$\lbrace$\textit{C}$\rbrace$相对于末端坐标系$\lbrace$\textit{E}$\rbrace$的坐标变换。一般情况下IBVS的伺服结果是相机正对目标，而真正抓取还是要依赖末端位置，所以这个变换是必要的；
$^{C}T_I$表示图像$\lbrace$\textit{I}$\rbrace$相对于相机坐标系$\lbrace$\textit{C}$\rbrace$的坐标变换。特征初始是在图像中获取的，需要这个变换使特征位置描述变成IBVS需要的形式\cite{zh1}。


\subsection{视觉模型建立}[Content specification]
IBVS不断地由特征偏差驱动着运行，而对特征的描述需要在$^{C}T_O$下进行。在图像中获取的特征需要经过图\ref{特征变换}所示的坐标系变换才能真正为IBVS所用：
\begin{figure}[h]
\centering
\includegraphics[width = 0.60\textwidth]{chapter2/特征变换}
\caption{特征变换过程图}
\label{特征变换}
\end{figure}


不例外地使用针孔模型描述从像素坐标系到机器人基坐标系中物体的映射,这张图引自这篇文献\cite{zh1}，这张针孔模型示意图十分典型。
\begin{figure}[h]
\centering
\includegraphics[width = 0.60\textwidth]{chapter2/视觉坐标系建立}
\caption{视觉坐标系建立示意图}
\label{视觉坐标系建立}
\end{figure}


图\ref{视觉坐标系建立}中用$X_CO_CY_C$描述$\lbrace$\textit{C}$\rbrace$，$xO_Iy$描述$\lbrace$\textit{I}$\rbrace$，P表示$\lbrace$\textit{C}$\rbrace$中的目标点，P'表示成像过程中投影到$\lbrace$\textit{I}$\rbrace$中的目标点。因为深度的存在，成像在二维像素坐标系中的图像所对应的目标可以是无穷多种情况，为了统一坐标变换形式，令目标深度$Z_C$为单位1，在相机坐标系$\lbrace$\textit{C}$\rbrace$和像素坐标系$\lbrace$\textit{CI}$\rbrace$中加上了一个过渡的图像坐标系$\lbrace$\textit{I}$\rbrace$。


相机内参由出厂地所给出，它包括相机的焦距$f$，相机放缩因子$f_x$和$f_y$，它们的单位为毫米；偏移量$c_x$和$c_y$，单位为像素，但是是浮点类型。由于Realsense D435i内置去畸变API，就不考虑畸变因素了。使用$\left[ u,v \right] ^T$表示像素坐标系下的目标点位置，$\left[ X_C,Y_C,Z_C \right] ^T$表示相机坐标系下的目标点位置，$Z_C$为相机深度，可以得到它们之间的关系：
\begin{equation}
	Z_C\left[ \begin{array}{c}
	\begin{array}{c}
	u\\
	v\\
\end{array}\\
	1\\
\end{array} \right] =\left[ \begin{matrix}
	f_x&		0&		c_x\\
	0&		f_y&		c_y\\
	0&		0&		1\\
\end{matrix} \right] \left[ \begin{array}{c}
	X_C\\
	Y_C\\
	Z_C\\
\end{array} \right] 
\label{像素到相机坐标变换} 
\end{equation} 


在已知相机内参的情况下，IBVS中所需要的特征位置描述就转换为找到特征对应的$\lbrace$\textit{CI}$\rbrace$中的位置$\left[ u,v \right] ^T$和深度相机测得对应点的深度$Z_C$。

\section{IBVS仿真系统实现}[Content specification]
\subsection{IBVS算法原理}[Content specification]
在本实验中所说的IBVS是基于特征点交互矩阵实现的\cite{chaumette2006visual}。它的基本思想是将特征点偏差通过交互矩阵（也叫图像雅可比矩阵）映射为末端速度指令。为了表述简洁，符号所代表的意思如下：$s^*=\left[ x^*,y^* \right] ^T$表示$\lbrace$\textit{C}$\rbrace$中对应的期望特征点坐标。$s=\left[ x,y \right] ^T$表示$\lbrace$\textit{C}$\rbrace$中对应的当前特征点坐标。$v_c=\left[ v_x,v_y,v_z,\omega _x,\omega _y,\omega _z \right] ^T$表示$\lbrace$\textit{B}$\rbrace$中相机的速度，其中相机包含质心线速度，和绕三个轴的角速度。由于相机和机械臂末端是固连且位置上接近的，所以它们的速度认为是一致的。使用交互矩阵$L_c$建立当前特征随时间变化率与相机位姿随时间变化率的关系：
\begin{equation}
\dot{s}=L_cv_c 
\label{当前特征变化率与相机位姿变化率的关系1} 
\end{equation} 


一般情况下，期望特征是不随时间改变的，或者变化甚微（在本研究中就是如此，所以进行近似），式\ref{当前特征变化率与相机位姿变化率的关系1}可以如方程组\ref{当前特征变化率与相机位姿变化率的关系2}中第一排表达式改写。另外，认为特征偏差随时间呈指数变化是合理的，因为它收敛快速且平滑\cite{chaumette2006visual}，于是可以得到方程组\ref{当前特征变化率与相机位姿变化率的关系2}：
\begin{equation}
\left\{ \begin{array}{c}
	\left( \dot{s}-\dot{s}^* \right) =\dot{e}=L_cv_c\\
	L_e=L_c\\
	\dot{e}=-\lambda e\\
\end{array} \right.  
\label{当前特征变化率与相机位姿变化率的关系2} 
\end{equation} 


其中$\dot{e}$为当前特征偏差随时间变化率，$\lambda$为比例系数，$\dot{e}$为当前特征偏差。通过对交互矩阵求广义逆，由方程组\ref{当前特征变化率与相机位姿变化率的关系2}可得到：
\begin{equation}
\left\{ \begin{array}{c}
	v_c=-\lambda L_{e}^{+}e\\
	L_{e}^{+}=\left( L_{e}^{T}L_e \right) ^{-1}L_{e}^{T}\\
\end{array} \right. 
\label{当前特征变化率与相机位姿变化率的关系3} 
\end{equation} 


其中$\lambda L_{e}^{+}$为交互矩阵广义逆。通过方程组\ref{当前特征变化率与相机位姿变化率的关系3}可以借助当前特征偏差求取机器人末端速度了。交互矩阵由特征点在图像中的位置及深度信息得到，每个点对应的交互矩阵如式\ref{交互矩阵}所示，若有多个点，公式中的交互矩阵就是每个点对应的交互矩阵在行方向的叠加。
 \begin{equation}
L_e=\left[ \begin{matrix}
	\frac{-1}{Z_C}&		0&		\frac{X_C}{Z_C}&		X_CY_C&		-\left( 1+X_{C}^{2} \right)&		Y_C\\
	0&		\frac{-1}{Z_C}&		\frac{Y_C}{Z_C}&		1+Y_{C}^{2}&		-X_CY_C&		-X_C\\
\end{matrix} \right] 
\label{交互矩阵} 
\end{equation} 

\subsection{机器人仿真模型搭建}[Content specification]
基于为整个系统搭建的坐标系和不同系的坐标转换关系，借助ROS的moveit工具（由于ROS2的moveit2尚未开发成熟，使用moveit代替），为敬科公司提供的JK机器人搭建仿真模型。moveit是一个开发的十分完善的工具包，不仅实现了机械结构的仿真，物理模型、碰撞体积和逆运动学都在包中相应地实现。本研究中，为了能更快地验证提出的算法，减少繁杂的处理，将把物体放到一个平整且颜色单一（在后续的研究中可以发现这些要求都不是必须的）的表面上。另外，相机一直保持俯视朝下，在X、Y轴方向的角度保持为0°，因此速度指令中$\omega _x$和$\omega _y$不论结果计算如何都给0。仿真效果图如图\ref{基于moveit机器人仿真模型实现}所示。
\begin{figure}[h]
\centering
\includegraphics[width = 0.55\textwidth]{chapter2/基于moveit机器人仿真模型实现}
\caption{基于moveit机器人仿真模型实现}
\label{基于moveit机器人仿真模型实现}
\end{figure}


图\ref{IBVS基础控制流程方框图}展现了整个系统最基础的控制方框图，在实验进行过程中会不断被改进，以应对实践中发生的各个问题。
\begin{figure}[h]
\centering
\includegraphics[width = 1.0\textwidth]{chapter2/IBVS基础控制流程方框图}
\caption{IBVS基础控制流程方框图}
\label{IBVS基础控制流程方框图}
\end{figure}


深度获取的方式是会根据深度条件切换的，如图\ref{深度传感器切换}所示。之所以加上这个切换，是因为Realsense D435i是基于结构光测量深度的，不可以测量过近距离的目标点。当相机在伺服末期十分靠近目标时，无法获得确切的目标点深度。所以在相机与目标点距离低于20cm时，会通过编码器读取末端下降的距离得到对应特征的深度，这在相机一直俯视向下时是可行的。


\begin{figure}[h]
\centering
\includegraphics[width = 0.60\textwidth]{chapter2/深度选择图}
\caption{深度传感器切换示意图}
\label{深度传感器切换}
\end{figure}


开启仿真节点后，可以在另一个节点中与该节点建立连接并发送速度指令，仿真节点会因此响应，并进行移动。moveit有自己的限位系统，在机器人进入奇异点或者超出移动范围时给予及时的警告，所以该机器人仿真模型多被用于对机器人是否进入奇异点的判断这样的定性分析，后文中真正的调参还是在实物上进行的。


接下来是对IBVS算法的仿真，通过末端速度指令曲线与相机轨迹来了解算法运行情况。IBVS是将特征偏差作为控制器输入而映射成速度指令的控制系统，所以研究中最关心的是点在于特征偏差和速度指令，它们将被分别绘制到两张图中。曲线图中时间单位为秒。关于特征偏差图：系统中定义特征偏差是相机坐标系中被检测的特征点在X、Y方向的偏差，单位为米，该单位不被展现在曲线中，因为它的单位并不重要。关于末端速度指令图：为了与JK机器人需要的末端速度指令单位保持一致，所以线速度选取厘米每秒为单位，而角速度单位则为度每秒。


直观地展现相机的位移情况也是重要的，因为IBVS往往对机器人末端的运动轨迹十分不友好。如果当前的速度指令使机器人颤振，那么机器人已经进入了一个十分糟糕的姿态，通过分析相机的运动轨迹适当调节控制律参数也是非常好的解决方法。调用VISP库，对设定的参数进行视觉伺服仿真，实现的曲线绘制和相机轨迹绘制效果如图\ref{基于VISP视觉伺服仿真}所示。
\begin{figure}[h]
\centering
\includegraphics[width = 1.0\textwidth]{chapter2/基于VISP视觉伺服仿真}
\caption{基于VISP视觉伺服仿真}
\label{基于VISP视觉伺服仿真}
\end{figure}


提前在程序中设置好特征的位置，和与之对应的目标特征。将仿真得到的速度指令通过ROS2节点发布订阅机制传输给机器人仿真节点，机器人会相应地运动并使当前特征都到达目标特征处，从而到成到达目标位置处的目的。
\section{IBVS实际系统实现}[Content specification]
\subsection{IBVS实物系统平台搭建}[Content specification]
仿真终归只能用于定性分析。外界干扰、噪声多种多样，仿真中不可能把所有因素考虑进去。事实上，仿真跑出的结果往往十分顺滑，而实物中会反映很多处理不够细节的问题。我认为，IBVS算法在实物上成功运行，研究才算真正的开始。实物运行环境包括JK机器人和装载它并固定它底座的台子;用于承载目标物体的平台和目标;平台上铺盖的一层漫反射效果好且为单一白色的纸;机器人末端装配Realsense D435i深度相机（夹具暂时未装配，在正式夹取的时候会安装在末端）。之所以要铺一层纸，除了保证平面平整且颜色单一以外，还保证了深度相机不要因为丢失反射光导致获取无效数据。最终实物环境图如图\ref{实物环境搭建展示图}所示：
\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter2/实物环境搭建展示图}
\caption{实物环境搭建展示图}
\label{实物环境搭建展示图}
\end{figure}

\subsection{IBVS实际运行}[Content specification]
为了能更快地验证IBVS算法，在平台上贴了一张黑色方框码，经过视觉二值化、边缘获取、多边形拟合等处理。实验进行前会将机械臂末端调到目标位置，此时黑色方框会处于摄像头的中央，记录此时的四个点为目标特征。将机器人末端初始位置调至远离黑色方框的位置，距离目标位置的三维各个方向以及Z轴角度都有一定的偏差（$\lbrace$\textit{B}$\rbrace$中，$\varDelta X=0.3m,\varDelta Y=0.3m,\varDelta Z=0.5m$）。伺服过程中会不断捕获它的四个点作为特征，并计算特征偏差，最后映射成末端速度指令。伺服的成功证实了所实现的IBVS算法的正确性，同时也正式踏入对未知物体视觉伺服抓取的研究领域中。\ref{视觉伺服曲线绘制（初始）}展示了伺服过程中机械臂末端速度指令和各特征点的偏差对应的曲线。
\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{chapter2/视觉伺服曲线绘制（初始）}
\caption{视觉伺服曲线（初始）}
\label{视觉伺服曲线绘制（初始）}
\end{figure}
\newpage
\section{本章小结}[Content specification]
本章讲述了基于特征点交互矩阵的IBVS的原理。在算法实现前建立系统坐标系和视觉坐标系，这会使之后的坐标描述便捷许多。搭建了IBVS的仿真运行环境，便于后续问题分析。搭建了实物运行环境，并成功运行了IBVS算法，这意味着研究真正的开始。
%%%%%%%%%%%%%%%%%%%%%%%%%%%抓取目标生成算法研究%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[伺服目标生成算法研究]{伺服目标自主生成算法研究}
\section{引言}[Content specification]
第二章完成了IBVS的模型建立、仿真系统搭建以及在实物上成功运行IBVS算法，这些为后续的研究提供了控制基础。现在机器人知道自己怎么动了，那么它面对未知物体抓取任务时如何知道自己往哪动呢？接下来，在本章中将对本次课题又一基石——伺服目标生成的研究进行展开。主要研究了基于模型的点云识别和抓取点生成网络两种方法，将它们进行比较后，选择了后者作为应对未知物体抓取任务时生成机器人伺服目标的方案。


\section{基于模型的点云识别与配准}[Content specification]
\subsection{方法陈述}
虽然物体样式多种多样，但它们总以一类一类的形式呈现，例如不同种类的苹果，形状会类似，苹果和球形状类似。对于每一类这样的物体称为一个模型，而对于每一个模型会有一个确定的抓取方式。先收集尽可能多的点云，计算它们的点云特征，对特征类似的进行聚类，计算每一类特征的平均值作为一个模型。每个模型会人为的设定点云中的部分特别位置的点作为期望特征。制作的其中一个圆柱类模型如图\ref{圆柱类点云模型}所示:
\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{chapter3/实时抓取点生成}
\caption{圆柱类点云模型}
\label{圆柱类点云模型}
\end{figure}


对于新来的未知物体，对每个模型进行ICP配准，解算当前物体相对模型的姿态，然后与原模型一致的方式提取当前点云特征，匹配模型的期望特征。将期望特征和提取的特征输入到IBVS控制器中完成伺服控制。


ICP即为迭代最近点法。假设有PA和PB两个点云，它们是相同物体的不同位姿时深度传感器获取的点云。该算法通过不断迭代它们之间的坐标变换矩阵参数的方式让两个点云尽可能的重叠\cite{chetverikov2002trimmed}。设PA和PB的点云分布分别为$p\left( x,y,z \right) $、$q\left( x,y,z \right) $，目前需要找到$P$到$Q$的旋转矩阵$R$和平移矩阵$T$，给出一个代价函数，通过最小二乘法求解最优解。在这之前，先表示出两点云的质心：
\begin{equation}
\left\{ \begin{array}{c}
	\vec{p}=\frac{1}{n}\sum_{i=1}^N{\vec{p}_i}\\
	\vec{q}=\frac{1}{n}\sum_{i=1}^N{\vec{q}_i}\\
\end{array} \right. 
\label{点云质心描述1} 
\end{equation}


然后从两个点集中的每个点减去相应的质心：
\begin{equation}
\left\{ \begin{array}{c}
	\vec{p}_t=\vec{p}_i-\vec{p}\\
	\vec{q}_t=\vec{q}_i-\vec{q}\\
\end{array} \right. 
\label{点云质心描述2} 
\end{equation}


则上述最优化目标函数可以转化为：
\begin{equation}
E=\sum_{i=1}^N{\left| \vec{q}_t-R\vec{p}_t \right|}
\label{ICP代价函数} 
\end{equation}


最优化问题最后分解为：
\begin{itemize}
\item[（1）]
求使代价函数$E$最小的旋转矩阵$R$。
\item[（2）]
求得平移矩阵$T=\vec{q}-R\vec{p}$。
\end{itemize}

\subsection{算法实现}
深度传感器获得的点云往往不全是目标物体的点云，在姿态匹配前，经过三个方向的点云截断滤波、去离群点、降采样操作，最后提取出目标点云。基于模板匹配点云识别过程中所提取的点云特征选取了VFH特征，它是一种全局特征，可以快速计算和匹配。算法的实现效果如图\ref{基于模型的点云识别实操图}和图\ref{基于模型的点云识别效果图}所示。


图\ref{基于模型的点云识别实操图}和图\ref{基于模型的点云识别效果图}分别是算法运行时第三方视角和电脑视角的图片，点云处理间隔为500ms，但识别和位姿解算间隔约为3s。
\newpage
\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter3/基于模型的点云识别实操图}
\caption{基于模型的点云识别实操图}
\label{基于模型的点云识别实操图}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter3/基于模型的点云识别效果图}
\caption{基于模型的点云识别效果图}
\label{基于模型的点云识别效果图}
\end{figure}
\subsection{方法小结}
该方法需要环境噪声较小时，才能正常匹配，消耗时间长，拖长了系统抓取物体所需运行的时间，且不可用于实时的目标特征生成，视觉伺服效果将会非常依赖初始的视觉、点云处理。另外，还存在模型制作困难、鲁棒性差等问题，这写让这个方法面对卷积神经网络完全没有一战之力。


\section{抓取点生成网络（GG-CNN）}[Content specification]
\subsection{方法陈述}
抓取点生成网络（GG-CNN）属于Grasp Synthesis中的一种，为PBVS量身订做的抓取点生成方案。Douglas Morrison等人在2018年提出了GG-CNN，该网络非常轻便快捷，可以通过输入深度图像，在19ms内输出图像中机器人的期望抓取位姿,最后依赖相机内参、外参计算出机械臂末端期望的位姿，通过PBVS来抓取未知物体\cite{morrison2018closing}。该工作最大的特点是，它能让视觉伺服实时生成期望位姿，伺服精度不再受初始计算的期望位姿影响。在后续研究中，会通过一个很特别而简单的方法将该网络所输出的结果应用于IBVS，但在这之前，先要弄清楚它的工作原理。


他们创新性在于提出了十分合适的网络输出。网络整体结构的设计是非常简单的，追求大感受野，然后就是很寻常的叠层。网络限定了机器人末端需要时二指的，视觉伺服控制中，相机必须保持时刻俯视，这也是在第二章实物搭建中这么做的原因之一。网络的本质是语义分割，输出4张与输入的300*300深度图像相同大小的图像$G=\left[ Q,W,\varPhi \left( \sin \theta ,\cos \theta \right) \right]$，其中使用$\sin \theta$和$\cos \theta$分别对应不同的输出图片。$Q$图像中每个像素代表这个点的抓取质量，它们都是被归一化的数据，1表示抓取质量很高，0表示这个点完全不值得抓取；$W$图像中每个像素代表抓取这个点所需要的二指张开宽度；$\varPhi$图像中每个像素代表抓取这个点所需要的二指沿Z轴旋转角度。


本文中认为他们所构建的网络有很大的优化空间。他们为了加快网络计算速度，所设计的层数太少，网络的非线性程度较低。所以使用1*1卷积层对网络非线性化。其次，作者非常喜欢使用大卷积核。实际上在机械臂末端运动过程中，物体在相机中的大小会有很大的改变，单一感受野并不能适应这样的变化，所以将大卷积核拆成不同尺度小卷积核的叠加。以上的优化并不会带来太多的计算量，因为改动的地方只是增加了1*1卷积层和拓展的小卷积核，它们本身不会带来什么计算量。最终网络如图\ref{GG-CNN改进}所示。
\begin{figure}[h]
\centering
\includegraphics[width = 1.0\textwidth]{chapter3/GG-CNN改进}
\caption{GG-CNN改进后网络架构}
\label{GG-CNN改进}
\end{figure}


\subsection{算法实现}
配好pytorch环境，使用Cornnel数据集。将计算出的最大抓取质量像素点作为长方形中心，$\varPhi$为长方形绕中心旋转角度，$W$作为长方形的长，长方形宽为它的1/2，绘制长方形框，将计算出的IoU（预测抓取框与标签抓取框对应交集与并集的比）作为预测准确率，选取数据的10\%作为测试集，跑通优化后的GG-CNN代码训练程序，约40 epoch时达到了对测试集的80\%准确率。这个准确率相对于原始程序上升了4\%，优化是有效的。对给定深度图，输出效果如图\ref{GG-CNN输出（初始）}所示：
\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter3/GG-CNN输出（初始）}
\caption{GG-CNN输出}
\label{GG-CNN输出（初始）}
\end{figure}


这里特别解释一下图\ref{GG-CNN输出（初始）}中图Q表示抓取质量热度图，颜色越偏暖色越值得被抓，图Angle表示抓取角度图，单位是弧度，可以看到对柱形物体，抓取角度在高抓取质量点处几乎一致。


在代码测试过程中发现Realsense D435i对深度的测量信噪比很低，测量深度波动很大，只依靠深度图像一阶图像矩无法稳定定位目标物体的位置，所以通过RGB图像和深度图像的一阶图像矩（如果能有更好的深度传感器，是不需要依赖颜色信息的）可以得到目标大致位置。使用这个作为中心对当前640*480的图像进行考虑边缘（如果超出原始图像范围，会平移中心）的300*300裁剪。将神经网络写成ROS2的一个节点，实时发布计算出的抓取点信息，视觉伺服节点会订阅它的主题，实时显示抓取方框。选择抓取对象为笔袋，图\ref{实时抓取点生成}为实时抓取点生成效果图。
\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter3/实时抓取点生成}
\caption{实时抓取点生成}
\label{实时抓取点生成}
\end{figure}


实时显示抓取框和之前只显示一次抓取框有很大的不同，因为神经网络输出的抓取质量在目标位置处会有很多相近的点，如果物体具有平移对称性或者旋转对称性，抓取点的位置会不断跳动，正如图\ref{神经网络原始输出}所示。这会使生成的目标特征不断摆动，导致系统失稳。（由于角度为弧度制过太小，输出时乘了100倍）
\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter3/神经网络原始输出}
\caption{神经网络原始输出}
\label{神经网络原始输出}
\end{figure}


在GG-CNN原文中给出的解决方式是计算抓取质量图中三个局部最大点，选择与上一次的抓取点最近的点作为本次的抓取点。但是研究中在复现他们的算法后依然无法解决波动很大的问题，甚至调高选择的局部最大点个数到10。本文认为这是物体平移、旋转对称性带来的必然结果，无论网络好坏，因为网络的输入是深度图像。所以该网络输出的抓取点位置信息无法使用，只能委曲求全使用RGB图像的一阶图像矩代替，毕竟在裁剪图像的时候就求取过一阶图像矩的值。颜色信息在所搭建的实验环境中是非常稳定的，这样稳定的抓取点中心可以被用于实时抓取点生成。只是使用该方法会带来另一个麻烦，在伺服末期，如果目标靠近平台边缘，摄像头将捕捉地面的颜色信息，会对抓取点中心的计算带来干扰，这个问题将在本文第四章和其它问题一并解决。


好在角度的波动能得到很好的解决。因为相机时刻俯视向下，所以相机期望的沿Z轴的旋转角度和抓取框旋转角度是一致的。对于这样一个线性系统，使用卡尔曼滤波可以有效抑制噪声。机械臂末端沿Z轴的角速度设为$\omega_z$，它由每次控制周期$\delta _t$的IBVS控制器计算出。机械臂末端旋转角度为$\theta_z$，可以通过读取机械臂编码器的值间接获取。可以写出状态、观测方程：
\begin{equation}
\hat{x}_{k+1}=\left[ \begin{array}{c}
	\theta _z\\
	\omega _z\\
\end{array} \right] _{k+1}=\left[ \begin{matrix}
	1&		0\\
	0&		\delta _t\\
\end{matrix} \right] \left[ \begin{array}{c}
	\theta _z\\
	\omega _z\\
\end{array} \right] _k=A\hat{x}_k
\label{状态方程}
\end{equation}
\begin{equation}
z_{k+1}=\theta _z=\left[ \begin{matrix}
	1&		0\\
\end{matrix} \right] \left[ \begin{array}{c}
	\theta _z\\
	\omega _z\\
\end{array} \right] _k=H\hat{x}_k
\label{观测方程}
\end{equation}


认为过程噪声$\omega_k$和观测噪声$\upsilon_k$都服从高斯分布：
\begin{equation}
\left\{ \begin{array}{c}
	p\left( \omega \right) \sim N\left( 0,Q \right)\\
	p\left( \upsilon \right) \sim N\left( 0,R \right)\\
\end{array} \right. 
\label{噪声分布}
\end{equation}


则有协方差矩阵的预测方程：
\begin{equation}
\hat{P}_{k+1}=AP_kA^T+Q
\label{协方差预测}
\end{equation}


设卡尔曼滤波的增益为$K$，则根据以上条件求得：
\begin{equation}
K_{k+1}=\hat{P}_{k+1}H^T\left( H\hat{P}_{k+1}H^T+R \right) ^{-1}
\label{卡尔曼滤波的增益}
\end{equation}


使用观测器数据对当前预测状态进行更新，对卡尔曼滤波器的初始化中，令$R$、$Q$都为单位矩阵：
\begin{equation}
x_{k+1}=\hat{x}_{k+1}+K_{k+1}\left( z_{k+1}-H\hat{x}_{k+1} \right) 
\label{卡尔曼滤波的增益}
\end{equation}


再使用新的预测的状态更新协方差矩阵：
\begin{equation}
P_{k+1}=\left( I-K_{k+1}H \right) \hat{P}_{k+1}
\label{更新协方差矩阵}
\end{equation}


在机械臂静止不动时，神经网络预测的抓取点中心、角度随时间变化曲线如图\ref{抓取点中心方法替代与角度滤波}所示。得到的抓取中心只有1到2像素点的波动，预测的抓取角度几乎不再波动。
\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter3/抓取点中心方法替代与角度滤波}
\caption{抓取点中心方法替代与角度滤波}
\label{抓取点中心方法替代与角度滤波}
\end{figure}

\subsection{方法小结}
GG-CNN是十分契合本次研究目的的研究成果，因此它被选作未知物体抓取算法中解决机械臂怎么去抓取目标的方法。但是在复现工作中遇到了抓取中心波动过大的无法解决的问题，只能使用RGB图像的一阶图像矩方法代替它进行抓取点中心生成，最终只保留了该网络输出的角度项，在经过卡尔曼滤波处理后拥有比较稳定的输出值。

\section{本章小结}[Content specification]
本章研究了两种伺服目标自主生成的算法，比较后，选择了后者作为后续研究生成伺服目标的主要方法。相比于传统的基于模型获取抓取点的方法，神经网络有更好的鲁棒性、实时性。但是目前选取的GG-CNN并不是完美的，在实时伺服中它的抓取点中心因为目标物体平移、旋转对称性的存在，波动大到无法使用滤波的方法来抑制了。所以使用RGB一阶图像矩来生成抓取点中心。对于网络输出的抓取角度使用线性卡尔曼滤波对波动进行有效的抑制。


%%%%%%%%%%%%%%%%%%基于GG-CNN和IBVS的未知物体抓取算法实现%%%%%%%%%%%%%%%%%%%
\chapter[未知物体抓取算法实现]{未知物体抓取算法实现}
\section{引言}[Content specification]
第二章、第三章分别解决了控制基础和控制目标自主生成的问题，在对它们研究的设计中无不为本章而服务。本章将正式进行未知物体抓取算法的实现。通常情况下，相机视野里在伺服过程中会出现平台以外的部分，这影响了神经网络的正常工作，将使用图像处理抑制这样的边界环境干扰。然后提出一种基于抓取点和ORB特征的IBVS，在失败中不断累计经验，在该思想上提出一种自定义特征完美实现GG-CNN对IBVS的内嵌并优化了它的特征点分布。最后对于笔盒和卷纸做了无形状、位姿先验信息的视觉伺服实验，伺服指令平滑而收敛证明了该方法的有效性。

\section{图像处理抑制边界环境干扰}[Content specification]
当抓取目标设置在平台边缘或者抓取角度较大时，视觉伺服过程中，相机视野中会出现平台以外的部分。而神经网络是通过平整的平台上摆放物体的深度图训练的，同时，场景深度的大幅度波动会使检测到的特征深度随之大幅度变化，导致控制指令存在发散的状况。图\ref{深度突变导致抓取点预测错误}为出现这种状况时抓取点生成情况，十分明显，生成了错误的抓取点。
\begin{figure}[h]
\centering
\includegraphics[width = 0.6\textwidth]{chapter4/深度突变导致抓取点预测错误}
\caption{深度突变导致抓取点预测错误}
\label{深度突变导致抓取点预测错误}
\end{figure}


为了解决这个问题，进行一定的图像处理是必要的。目前掌握的信息有RGB图和深度图，深度图在之前的实验中已经发现了噪信比大的问题，并且存在许多接受的无效值导致的空洞。首先使用OpenCV所带的图像修复函数，选择Alexandru Telea发表的“基于快速行进方法的图像修复技术”方法\cite{telea2004image}，对所有无效值区域图像修复，修复前后对比如图\ref{深度图修复前后对比}所示。左图是未经过修复的深度图像，它左边大块的黑色区域是深度图向彩色图配准时该位置被遮挡而测量得到的无效值，中间空洞处是深度相机测量机制得到的测量无效值。右图是修复后的深度图像，还是会有一些空洞（它是随机出现的，左图的空洞一直会存在），但是它们都出现在深度突变的地方，对目标附近的深度获取是没有影响的。
\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{chapter4/深度图修复前后对比}
\caption{深度图修复前后对比}
\label{深度图修复前后对比}
\end{figure}


深度图像经过处理变得十分可靠。注意到平台以外的部分为蓝色的地面和银色的机械臂基座，接下来的思路就是确定平台位置以及对应的深度，通过判断是否大于平台深度来排除地面和基座。可以通过颜色区分平台和地面，但是机械臂基座和平台颜色相近，二值化后使用颜色中心来确定平台位置并不合适。为此，在对RGB图像二值化后，取对应白色区域的深度图像的深度中值作为平台的深度。因为机械臂再怎么运动都不会离开平台过多，离开平台过多情况的出现往往意味着视觉伺服的失败，所以基座对应的RGB图像的范围相比平台是更小的，那么取中值能有效地找到平台对应的深度。最后将大于这个深度0.5cm位置的深度全部置为平台深度，这样就会把边界的深度都通过平台深度拓展，处理过程见图\ref{图像处理抑制边缘干扰}，图a是原始RGB图像，图b是提取的平台对应像素，图c是算法分割出背景后将背景置为白色的RGB图像，图d是算法分割出背景后将背景置为平台深度的深度图像。
\newpage
\begin{figure}[h]
	\centering
	\subfigure[]{
		\begin{minipage}[h]{0.45\textwidth}
			\includegraphics[width=1\textwidth]{chapter4/图像处理抑制边缘干扰a} 
		\end{minipage}
		\label{图像处理抑制边缘干扰a}
	}
    	\subfigure[]{
    		\begin{minipage}[h]{0.45\textwidth}
   		 	\includegraphics[width=1\textwidth]{chapter4/图像处理抑制边缘干扰b}
    		\end{minipage}
		\label{图像处理抑制边缘干扰b}
    	}
	\\ 
	\subfigure[]{
		\begin{minipage}[h]{0.45\textwidth}
			\includegraphics[width=1\textwidth]{chapter4/图像处理抑制边缘干扰c} 
		\end{minipage}
		\label{图像处理抑制边缘干扰c}
	}
    	\subfigure[]{
    		\begin{minipage}[h]{0.45\textwidth}
		 	\includegraphics[width=1\textwidth]{chapter4/图像处理抑制边缘干扰d}
    		\end{minipage}
		\label{图像处理抑制边缘干扰d}
    	}
	\caption{图像处理抑制边缘干扰}
	\label{图像处理抑制边缘干扰}
\end{figure}



\section{初步思路}[Content specification]
\subsection{ORB特征的提取与匹配}[Content specification]
基于特征点交互矩阵的IBVS依赖特征的提取与匹配来完成伺服任务。这需要要求特征点的提取与匹配具有很强的稳定性。不仅如此，式\ref{交互矩阵}不具备特征点顺序的对称性，这要求特征点匹配时需要保证顺序一致，否则速度指令会出现锯齿状的波形。研究中做了一次仿真，选择四个特征点，不断改变特征点对应的顺序，得到的速度指令波形如图\ref{特征变换指令波动}所示。指令存在较大波动，所以尽可能保证交互矩阵中的特征点顺序不要改变。
\begin{figure}[h]
\centering
\includegraphics[width = 0.7\textwidth]{chapter4/特征变换指令波动}
\caption{特征变换指令波动}
\label{特征变换指令波动}
\end{figure}


针对以上对特征点性质的要求，选用平移不变性和旋转不变性的ORB特征，它基于金字塔结构提取，对于图像的缩放具有一定的鲁棒性。调用OPenCV的ORB特征提取API，记录每次提取的特征及其描述子并在下次用于匹配。匹配的特征点必须保证描述子汉明距离和两点在图像中的位置相差在一定阈值内，否则认为特征点丢失，从特征点候选中取出来补充，候选特征点个数一直处于20个（实际用于运算的特征点数为4个），如图\ref{ORB特征点提取与匹配}中图a所示。
\begin{figure}[h]
	\centering
	\subfigure[ORB特征点提取候选点]{
		\begin{minipage}[h]{0.45\textwidth}
			\includegraphics[width=1\textwidth]{chapter4/ORB特征点提取候选点}
		\end{minipage}
		\label{ORB特征点提取候选点}
	}
    	\subfigure[ORB特征点匹配点聚集现象]{
    		\begin{minipage}[h]{0.45\textwidth}
   		 	\includegraphics[width=1\textwidth]{chapter4/ORB特征点匹配点聚集现象}
    		\end{minipage}
		\label{chapter4/ORB特征点匹配点聚集现象}
    	}
	\caption{ORB特征点提取与匹配}
	\label{ORB特征点提取与匹配}
\end{figure}


但是将该算法实时运行时，出现了特征分布收缩的问题，而且仅仅在30帧内特征点会彻底聚集在一起。如图\ref{ORB特征点提取与匹配}中图b所示。经过后续的分析，认为这是必然的趋势。纯粹的依靠图像特征提取与匹配，加上距离的约束，那么4个几乎相同的特征点是最稳定的解，它们相似而又相近。
\subsection{基于抓取点的IBVS}[Content specification]
第三章中实现了一阶图像矩和GG-CNN实时输出抓取点。为了能将该信息使用于IBVS中，基于这个抓取点和相机一直保持俯视的条件，提出一种基于抓取点的IBVS。总特征点数还是4，会时刻计算抓取点附近的特征点并通过抓取点信息自主生成的目标特征，特征点丢失会补充。假设在深度测量器从深度相机向编码器切换后，特征点不会丢失。


首先是目标特征$s^*$自主生成。认为机器人到达了期望位姿时，抓取点应当在图像的中心位置，那么由于相机俯视，此时检测到的特征$s$转化为$s^*$的过程实际上就是一个平面上平移、旋转和放缩的过程。如图\ref{俯视相机成图}所示，这是俯视相机所看到的世界，想把图中相机（三维坐标轴）变换到正对Apritag码（模拟抓取点）中心的位置，只需要在XY平面上平移，然后按照网络输出的Z轴旋转角度沿Z轴旋转，最后沿Z轴平移到期望深度即可。
\begin{figure}[h]
\centering
\includegraphics[width = 0.65\textwidth]{chapter4/俯视相机成图}
\caption{俯视相机成图}
\label{俯视相机成图}
\end{figure}


由于相机俯视，在图像中的点经历的流程和相机本身是一样的。将抓取点移到图像中心的平移量赋给当前检测到的特征点。设抓取点在图像中的位置为$\left( u_g,v_g \right)$，需要旋转的角度为$\theta$，设图像大小为$\left( w,h \right)$，先将图像坐标系移到按照$\left( 0,0 \right)$对称的坐标系，平移量为：
\begin{equation}
T_0=\left[ -w/2,-h/2 \right] ^T
\label{目标特征转化平移量}
\end{equation}


对抓取点平移量：
\begin{equation}
T_1=\left( \varDelta x,\varDelta y \right) =\left[ w/2-u_g,h/2-v_g \right] ^T
\label{目标特征转化抓取点平移量}
\end{equation}


总平移量为：
\begin{equation}
T=T_0+T_1=\left[ -u_g,-v_g \right] ^T
\label{目标特征转化总平移量}
\end{equation}


让此时特征点沿原点旋转，旋转矩阵：
\begin{equation}
R=\left[ \begin{matrix}
	con\left( \theta \right)&		-\sin \left( \theta \right)\\
	\sin \left( \theta \right)&		\cos \left( \theta \right)\\
\end{matrix} \right] 
\label{目标特征转化旋转矩阵}
\end{equation}


设目标特征$s^*$，目标深度$Z_d$。设当前特征$s=\left[ u,v \right] ^T$，当前特征深度均值$\bar{Z}$。期望特征在图像中的位置可以计算：
\begin{equation}
s^*=\frac{\bar{Z}}{Z_d}\left( R\left( s+T \right) -T_0 \right)
\label{目标特征生成}
\end{equation}


在笔袋上贴上黑色方框，将方框四个点作为特征来计算目标特征。最终实现效果如图\ref{目标特征生成图}所示，左边蓝色的四个点是根据抓取点生成的四个目标点，在机械臂处于期望位姿时，当前特征点应到处于这个位置。
\begin{figure}[h]
\centering
\includegraphics[width = 0.7\textwidth]{chapter4/目标特征生成}
\caption{目标特征生成}
\label{目标特征生成图}
\end{figure}


但非常遗憾的是，该方法跑出的伺服指令曲线是发散的，如图\ref{指令发散现象}所示。究其原因在于抓取点生成状态的波动对目标特征的生成影响是很大的。因为机械臂末端处于初始位置时，当前特征比较聚集，生成的目标特征在沿Z轴放大后相对误差是很大的。另外，在当前特征的提取和目标特征生成算法下，当前特征和目标特征的位置和旋转角度同时在改变，它们的误差将叠加。
\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter4/指令发散}
\caption{指令发散现象}
\label{指令发散现象}
\end{figure}


\subsection{总结与思考}[Content specification]
初步的想法无法实现对于未知物体抓取的任务。但它们的失败为后续给出的解决方法累积了经验。ORB特征匹配无法持续，是因为特征提取自RGB图像的算法中，匹配的最稳定解就是所有特征点相同。而摄像头是移动且具有噪声的，这更是加剧了解向这个方向收敛，所以基于图像的特征提取对于快速伺服是不可行的。通过抓取点计算目标特征来运行IBVS的想法也遭遇了困境，它失败的核心原因在于当前特征点和目标特征点的位置和角度同时在改变，误差会累加。另外，没有把控特征点的分布，特征点具有的信息不全面，所以伺服效果不会好。
\section{一种基于抓取点的自定义特征}
\subsection{方法概述}[Content specification]
图像提取特征不可取，既然知道抓取点的位置，通过人为定义特征点相对于抓取点的位置分布可以保证特征提供位置信息的全面性。固定目标特征的位置，保证只有特征的位置在改变，这样不会出现严重的误差叠加现象。选取的当前特征和目标特征分布如图\ref{基于抓取点的4x4自定义特征}所示。当前特征相对于目标特征的改变在于它经历了旋转和放缩。旋转矩阵$R$由GG-CNN提供，并且表达式如式\ref{目标特征转化旋转矩阵}，放缩因子$m$为期望的与物体表面的深度$h_d$和当前与物体表面深度$\bar{h}$的比值。
\begin{figure}[h]
	\centering
	\subfigure[4x4自定义当前特征]{
		\begin{minipage}[h]{0.45\textwidth}
			\includegraphics[width=1\textwidth]{chapter4/44自定义当前特征}
		\end{minipage}
		\label{44自定义当前特征}
	}
	\subfigure[44自定义目标特征]{
		\begin{minipage}[h]{0.4\textwidth}
			\includegraphics[width=1\textwidth]{chapter4/44自定义目标特征}
		\end{minipage}
		\label{44自定义目标特征}
	}
	\caption{4x4自定义当前特征}
	\label{基于抓取点的4x4自定义特征}
\end{figure}


可以通过深度相机测量当前特征点对应的深度，那么现在只剩下目标特征深度取何值的问题。将使用一种迭代的方式求取物体表面深度$\bar{h}$，结合当前特征深度$h$与这个值的差值以及预设的机械臂在期望位姿时距离物体表面的深度$h_d$求取目标特征深度。目标特征深度$h^*$表达式如下：
\begin{equation}
h^*=h-\bar{h}+h_d
\label{目标特征深度}
\end{equation}


可以求得当前特征位置：
\begin{equation}
s=mR\left( s^*+T_0 \right) -T
\label{当前特征位置}
\end{equation}


这样特征和目标特征的信息就完整了，将它们给予IBVS控制器即可完成伺服。称这种特征叫做基于抓取点的自定义特征，因为这个特征分布排列是自定义的。
\subsection{目标表面计算}[Content specification]
为了求取目标特征深度，必须知道物体表面的深度，距离物体表面一定距离时给机械臂下达停止指令，准备抓取。但是对于一个未知形状的物体，它可以是圆锥这种不存在平面表面的物体，单取物体对应像素处的深度的平均值来作为物体表面深度是不合适的。所以使用一种迭代的方式，不断滤除大于当前物体深度平均值的像素点，取剩余像素点深度的平均值继续滤除。迭代示意如图\ref{迭代示意图}所示。算法效果相当于不断通过平均深度平面截断目标物体，获得平面以上的部分继续求平均深度，然后重复以上步骤。
\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter4/迭代示意图}
\caption{迭代示意图}
\label{迭代示意图}
\end{figure}


实验验证重复两次是合适的，继续迭代所获得的物体表面深度改变在1mm左右，已经到达深度相机的深度分辨率了。另外，因为滤除次数过多会导致计算量增大，剩余点数较少，受噪声的影响变大。对于卷纸的迭代前后物体像素对比如图\ref{迭代求前景前后对比}所示。该图将笔袋竖着放置，左图为迭代前经过图像处理得到的笔袋对应的像素，右图为迭代后对应的像素。经过迭代后，用于求平均深度对应的像素只有笔袋的上半部分。获得的物体表面深度为0.4464m，由卷尺测量为0.4460m，毫米级的误差证明了算法的有效性。
\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter4/迭代求前景前后对比}
\caption{迭代求前景前后对比}
\label{迭代求前景前后对比}
\end{figure}


在运行IBVS程序，时遇到了Z轴速度指令一直接近0的问题，不论抓取对象是笔袋还是卷纸。指令曲线如图\ref{速度指令接近0的情况}所示。经过反复实验发现只要特征分布恰好全在现实世界的一个平面上且旋转角度偏差很小的时候就会导致这个问题。对于笔袋，当前设定的特征分布相对聚集，所有特征点都在笔袋上；对于卷纸，当前设定的特征分布相对松散，所有特征点都在平台上，所以需要优化当前特征分布。\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{chapter4/Z轴速度接近0的情况}
\caption{Z轴速度接近0的情况}
\label{Z轴速度接近0的情况}
\end{figure}
\subsection{优化特征分布与IBVS试验}[Content specification]
当前特征需要涵盖平台、物体和物体细节的位置信息，最终设计的特征分布如图\label{优化自定义目标特征}所示。这样的设计保证了特征点能针对任意常见物体获取更多位置信息，Z轴速度指令回归正常。将笔袋作为抓取对象，执行IBVS程序，伺服效果如图\ref{优化自定义效果}所示。
\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\textwidth]{chapter4/优化自定义目标特征}
	\caption{优化自定义目标特征}
	\label{优化自定义目标特征}
\end{figure}


注意到在图\ref{伺服指令曲线}中，整个伺服过程所消耗的时间长达25s,此时的控制系统只能保证最基础的稳定性而已，在快速性方面有待提高。另外值得一提的是，所提出的特征分布不一定拘泥于GG-CNN算出的抓取点，对于俯视二指爪抓取物体的问题，任意算法给出一个抓取点，它都可以当一个插件挂上去，帮助运行IBVS。
\begin{figure}[h]
	\centering
	\subfigure[伺服结束相机视角]{
		\begin{minipage}[h]{0.4\textwidth}
			\includegraphics[width=1\textwidth]{chapter4/优化自定义效果a}
		\end{minipage}
		\label{伺服结束相机视角}
	}
	\subfigure[伺服指令曲线]{
		\begin{minipage}[h]{0.5\textwidth}
			\includegraphics[width=1\textwidth]{chapter4/优化自定义效果b}
		\end{minipage}
		\label{伺服指令曲线}
	}
	\caption{优化特征分布后伺服效果}
	\label{优化自定义效果}
\end{figure}
\section{本章小结}
本章首先通过图像处理解决了边界环境干扰问题，然后结合了前两章的研究成果提出了一种基于抓取点的IBVS，但是在实现过程中遭遇了失败。结合失败的经验，在当前研究成果上改进特征提取方法，提出一种基于抓取点的自定义特征，避免了特征和目标特征都在改变导致的误差叠加问题的同时，特征的排列顺序和匹配是可靠性问题也得以解决。



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%IBVS控制律优化%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[IBVS控制律优化]{IBVS控制律优化}
\section{引言}[Content specification]
之前的研究已经完成了机械臂基于视觉伺服的未知物体抓取算法，但是伺服性能是较差的。优秀的控制律会让视觉伺服系统精确、快速地收敛，这不仅会让抓取算法具有实用性，而且还能配合抓取算法提升系统抓取物体的成功率。本章将建立IBVS控制模型，使用PD控制器和自适应IBVS算法\cite{mansard2007task}提升伺服性能，使用滑摸控制器让系统对深度测量的不确定性具有鲁棒性，然后基于Lyapunov直接法证明系统稳定性。最后注意到特征偏差关联速度不一致性，利用特征偏差之间的相对大小改写交互矩阵，实现了一种自适应控制算法，使所有特征与最快收敛的特征同时收敛，提升了收敛速度。


\section{PD控制器与自适应IBVS}[Content specification]
在优化前，要搭建IBVS系统控制模型，了解原始IBVS控制律的本质，然后在它的基础上更改控制方法。根据式\ref{当前特征变化率与相机位姿变化率的关系1}和式\ref{交互矩阵}可以得到以下表达式：
\begin{equation}
\dot{e}=\left[ \begin{matrix}
	\frac{-1}{Z_C}&		0&		\frac{X_C}{Z_C}&		X_CY_C&		-\left( 1+X_{C}^{2} \right)&		Y_C\\
	0&		\frac{-1}{Z_C}&		\frac{Y_C}{Z_C}&		1+Y_{C}^{2}&		-X_CY_C&		-X_C\\
\end{matrix} \right] v_c
\label{IBVS展开式}
\end{equation}


式\ref{IBVS展开式}是单特征点对应的IBVS原理公式，而至少四个特征点才能保证稳定性\cite{chaumette2006visual}，在本研究中设选取特征点数为$n$，特征点分别为$P_1$、...、$P_n$，那么表达式改写成以下形式：
\begin{equation}
\left\{ \begin{array}{c}
	\dot{\varepsilon}_n=L_{en}v_c\\
	\dot{\varepsilon}_n=\left[ \dot{e}_1,...,\dot{e}_n \right] ^T\\
	L_{en}=\left[ \begin{array}{c}
	L_{e|P=P_1}\\
	...\\
	L_{e|P=Pn}\\
\end{array} \right]\\
\end{array} \right.  
\label{IBVS多特征点展开式}
\end{equation}


在原始公式中设计了特征偏差随时间呈指数衰减，从而得到式\ref{当前特征变化率与相机位姿变化率的关系3}中速度指令的表达式，将它改写成式\ref{当前特征变化率与相机位姿变化率的关系3改写}，它类似于设计了一个比例控制器，外面通过交互矩阵广义逆将特征偏差向末端速度指令映射。所以IBVS控制律的本质在于设计速度末端和特征偏差的关系。
\begin{equation}
v_c=L_{e}^{+}\left( -\lambda e \right) 
\label{当前特征变化率与相机位姿变化率的关系3改写}
\end{equation}

\subsection{PD控制器设计}[Content specification]
那么目前可以进行的优化在于原始公式中对应的比例项那一块。PID控制器被广泛应用于工业控制，实用可靠且简易。将原始的比例项改写成比例加微分项，其中它们的系数$K_p$和$K_d$都是正定矩阵。系数扩维会给调参制造麻烦，但是更为灵活，如果想要好的伺服性能，需要这一步。
\begin{equation}
v_c\left( t \right) =L_{e}^{+}\left( -K_pe\left( t \right) -K_d\dot{e}\left( t \right) \right) 
\label{PD控制律}
\end{equation}


同样的将笔袋作为抓取对象，放置位置使基坐标系下机械臂末端坐标系相对于物体坐标系的变换为$\varDelta X=0.3m,\varDelta Y=0.3m,\varDelta Z=0.5m,\varDelta \theta _x=0\degree,\varDelta \theta _y=0\degree,\varDelta \theta _z=15\degree$，本章中之后的试验都将保持这个相对位姿进行。伺服过程会在特征偏差的均方差小于$2*10^{-4}$时强制停止，从开始到此时的时间计为系统调节时间。为了能让抓取能成功进行，这是伺服精度的必须要求，没达到这个要求的都认为是失败的伺服，所以后续不再讨论精度的问题，而主要注重超调和调节时间。伺服曲线包括末端速度指令曲线和特征偏差曲线，其中特征偏差记录了12个特征点的$X$偏差和$Y$偏差，选取的点和初始情况下，笔袋在像素坐标系中的状态如图\ref{实验前准备}所示，图\ref{绘制曲线选取特征点}中红色的点为选取点。
\begin{figure}[h]
	\centering
	\subfigure[笔袋在相机视野中初始位置]{
		\begin{minipage}[h]{0.4\textwidth}
			\includegraphics[width=1\textwidth]{chapter5/笔袋初始位姿}
		\end{minipage}
		\label{笔袋在相机视野中初始位置}
	}
	\subfigure[绘制曲线选取特征点]{
		\begin{minipage}[h]{0.36\textwidth}
			\includegraphics[width=1\textwidth]{chapter5/绘制曲线选取特征点}
		\end{minipage}
		\label{绘制曲线选取特征点}
	}
	\caption{实验前准备}
	\label{实验前准备}
\end{figure}


参数调节好后将控制器应用于实物中运行，伺服曲线如图\ref{视觉伺服曲线（PD）}所示。指令曲线相较于图\ref{优化自定义效果}中的曲线要完善很多，调节时间从25s到15s，指令振荡在微分环节的作用下减弱许多。$X$方向特征偏差最大超调为0.2，$Y$方向特征偏差最大超调为0.2，控制系统还是存在超调现象，伺服速度还有优化的空间。
\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\textwidth]{chapter5/替身}
	\caption{视觉伺服曲线（PD）}
	\label{视觉伺服曲线（PD）}
\end{figure}

\subsection{自适应IBVS}[Content specification]
为了进一步提升IBVS系统的伺服性能，借鉴了这篇文献的算法思想\cite{mansard2007task}。该文献中中利用了特征偏差初值信息，放大了伺服系统在初期的指令，让伺服系统在一开始就快速收敛，并且在伺服过程中让这个放大效果快速衰减，不会对系统稳定性造成影响。所以继续改写式\ref{PD控制律}，得到下式。其中$k_a$为自适应项系数，它是一个常数，$T$为系统控制周期。
\begin{equation}
v_c=L_{e}^{+}\left( -K_pe\left( t \right) -k_ae\left( 0 \right) \exp \left( -\frac{t}{T} \right) -K_d\dot{e}\left( t \right) \right)  
\label{自适应IBVS}
\end{equation}


式\ref{自适应IBVS}中的自适应项本质是惯性环节，但是它的系数是可变的，取自该特征偏差对应的初值，所以大的初始偏差对应特征的收敛将会被加快。这一项会在伺服过程中快速衰减至0，伺服末期控制律依然按照式\ref{PD控制律}进行。将算法应用于实物中运行，得到的伺服曲线如图\ref{视觉伺服曲线（自适应）}所示：
\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\textwidth]{chapter5/替身}
	\caption{视觉伺服曲线（自适应）}
	\label{视觉伺服曲线（自适应）}
\end{figure}


由伺服曲线可以看出相较于图\ref{视觉伺服曲线（PD）}中的曲线，调节时间从15s变为8s，伺服速度提升了很多。$X$方向特征偏差最大超调为0.05，$Y$方向特征偏差最大超调为0.05，系统超调变得更加微弱，伺服性能已经达到了较高的水平（这一点在第六章与相关工作比较时会明显体现出）。但是在伺服曲线中存在部分的波动，尤其是在伺服末期，这是深度测量不确定性带来的，将在下一节使用滑模控制器进行解决。

\section{滑模控制器}[Content specification]
在伺服系统运行过程中，之所以存在指令波动，是因为对物体表面深度的估算存在波动。由于物体表面深度决定了当前自定义特征点分布的松紧情况，如图\ref{自定义特征点分布松紧程度波动}所示，这将导致特征偏差跟随波动，最后使指令波动。
\begin{figure}[h]
	\centering
	\subfigure[自定义特征点分布（正常）]{
		\begin{minipage}[h]{0.4\textwidth}
			\includegraphics[width=1\textwidth]{chapter5/替身}
		\end{minipage}
		\label{自定义特征点分布（正常）}
	}
	\subfigure[自定义特征点分布（波动）]{
		\begin{minipage}[h]{0.4\textwidth}
			\includegraphics[width=1\textwidth]{chapter5/绘制曲线选取特征点}
		\end{minipage}
		\label{自定义特征点分布（波动）}
	}
	\caption{自定义特征点分布松紧程度波动}
	\label{自定义特征点分布松紧程度波动}
\end{figure}


观察式\ref{当前特征变化率与相机位姿变化率的关系3改写}，所设计的控制律就是括号中的部分，而交互矩阵的广义逆就是对IBVS系统性质的描述，它是本章研究中真正的控制对象，而并非机械臂本身（这并不在本文的研究范围中）。它是抽象的并且模型和特征点位置有关的对象。对物体表面深度估计的不确定性导致了交互矩阵的变化，使系统模型产生摄动。所以现在要解决的实际上是模型摄动的问题。在多项研究成果中都表明了滑模控制对系统鲁棒性的提升有着积极的作用\cite{yuksel2015ibvs,parsapour2013position}，尤其是系统模型或者参数模型存在不确定性的时候，因为滑模控制通过系统状态的情况切换系统控制律以保证系统应对各个情况都有相应的处理办法。


物体表面深度估算的波动会使部分特征的偏差出现不确定的变大或变小，这将影响PD控制器的工作。所以使用滑模控制器\cite{li2018enhanced}抑制这个影响，定义滑动面$sur\left( t \right) =e\left( t \right) =s\left( t \right) -s^*\left( t \right) $，特征偏差到达滑动面会一直停留在该处。改造公式\ref{自适应IBVS}如式\ref{滑模控制器}。使用了饱和函数代替符号函数减弱滑模控制带来的颤动。
\begin{equation}
v_c=L_{e}^{+}\left( -K_pe\left( t \right) -k_ae\left( 0 \right) \exp \left( -\frac{t}{T} \right) -K_d\dot{e}\left( t \right) +K_ssat\left( sur\left( t \right) \right) \right) 
\label{滑模控制器}
\end{equation}


图\ref{视觉伺服曲线（滑模控制）}是将该算法应用于实物运行时的伺服曲线。系统调节时间为8s。$X$方向特征偏差最大超调为0.04，$Y$方向特征偏差最大超调为0.04，这与之前变化不大，但是从指令曲线上可以明显看出波动减少了，物体表面深度估算的波动带来的影响被有效抑制。
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{chapter5/替身}
	\caption{视觉伺服曲线（滑模控制）}
	\label{视觉伺服曲线（滑模控制）}
\end{figure}
%\section{系统稳定性}[Content specification]
%下面利用李雅普诺夫直接法证明当前系统的稳定性，并非完全严格，有一定近似成分。假设在当前IBVS控制律下，特征偏差会收敛到0，并且认为自适应项系统控制周期相对于系统调节时间非常小。李雅普诺夫方程和它的导数如下所示：
%\begin{equation}
%\left\{ \begin{array}{c}
%	V=\frac{1}{2}e^Te\\
%	\dot{V}=e^T\dot{e}\\
%\end{array} \right. 
%\label{李雅普诺夫方程}
%\end{equation}
%
%
%$\dot{e}$与速度$v_c$的关系由式\ref{当前特征变化率与相机位姿变化率的关系1}给出，并且根据设计的控制器的表达式\ref{滑模控制器}可以得到下式：
%\begin{equation}
%\dot{e}=L_{en}L_{en}^{+}\left( -K_pe\left( t \right) -k_ae\left( 0 \right) \exp \left( -\frac{t}{T} \right) -K_d\dot{e}\left( t \right) -K_ssat\left( sur\left( t \right) \right) \right) 
%\label{特征偏差导数表达式}
%\end{equation}


\section{特征偏差收敛速度不一致性}[Content specification]
对图\ref{视觉伺服曲线（滑模控制）}放大特征偏差曲线细节（以X方向偏差为例）得到图\ref{特征偏差细节放大（X方向）}。可知，每个特征偏差收敛的速度不一致，且特征偏差初值大的收敛速度更慢，这就是特征偏差收敛速度不一致性问题。原始视觉伺服控制律将所有偏差对偏差随时间变化率的影响一视同仁，事实上这是不合理的，那些收敛速度较慢的特征会对伺服速度造成不良影响，如果使它们的作用效果随着自己的大小做出改变并与收敛速度最快的特征同时收敛，会一定程度上提升伺服性能。
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{chapter5/替身}
	\caption{特征偏差细节放大（X方向）}
	\label{特征偏差细节放大（X方向）}
\end{figure}


注意到式\ref{当前特征变化率与相机位姿变化率的关系1}，假设末端速度保持不变，每一个特征偏差随时间变化率和图像雅可比矩阵的对应行有着直接对应关系。根据当前想法提出一个自适应的方法，对图像雅可比矩阵的进行改造。应当使偏差更大的特征有着更快的变化率。注意到偏差初值不同的特征，它们抵达稳定状态所消耗的时间是随它们的偏差大小是非线性增加的，所以采用指数来扩大它们的影响更为恰当。所以对图像雅可比矩阵改造如下：
\begin{equation}
\left\{ \begin{array}{c}
	L_{enew}=K_bL_e\\
	K_b=\exp \left( \beta \left( \frac{e^T}{\min \left( e \right)}-1 \right) \right)\\
\end{array} \right.
\label{特征偏差收敛速度不一致性}
\end{equation}


这里的取指数是指对列维度的广播，并非对矩阵求指数。这么做让偏差最小的特征控制律保持不变，其它特征偏差越大，给出相同末端速度指令情况下会使该特征的变化速度更大。另外，这个算法是自适应的，只需要对$\beta$一个参数进行调节。图\ref{视觉伺服曲线（最终效果）}是将该算法应用于实物运行时的伺服曲线。最终在$X$方向特征偏差最大超调为0.01，$Y$方向特征偏差最大超调为0.01情况下，做到了4s的调节时间，伺服性能提升是显而易见的。
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{chapter5/替身}
	\caption{视觉伺服曲线（最终效果）}
	\label{视觉伺服曲线（最终效果）}
\end{figure}


\section{本章小结}[Content specification]
本章在当前抓取算法下对视觉伺服系统的控制律进行优化。PD控制器和自适应IBVS算法能有效提升伺服性能，滑摸控制器一定程度抑制了深度相机对深度测量的不确定性，然后基于Lyapunov直接法证明系统稳定性。针对出现的特征偏差关联速度不一致性，提出了一种自适应控制算法，通过使所有特征与最快收敛的特征同时收敛，带来了收敛速度的提升。经过对控制律的不断优化，最终系统控制示意框图确认为图\ref{控制示意框图}所示。
\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{chapter5/控制示意框图}
	\caption{控制示意框图}
	\label{控制示意框图}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%实验设计与验证%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{实验设计与验证}
\section{引言}[Content specification]
前面的章节中详细说明了通过视觉伺服抓取未知物体的思路、视觉伺服控制律的优化以及它们的实现过程。在前人的研究成果基础上，这套未知物体抓取算法提出创新性的特征点提取、匹配方式和与之配套的视觉伺服控制律。它将在更少的环境条件限制下拥有更高的未知物体抓取率和更理想平滑的曲线。本章将对一共六个家常物体，每个物体六种位姿进行抓取实验，然后将抓取成功率与各生成抓取合成类方法进行比较，证明这套未知物体抓取算法的优越性。另外，会对不同物体同一位姿、相同物体不同位姿的伺服曲线进行记录，通过超调、响应时间等参数反映算法应对多种多样情况下的高鲁棒性。

\section{实验设计}[Content specification]
对于未知物体抓取任务，抓取成功率是最需要关注的指标。为了使成功率的测试结果能体现系统应对各种情形下的抓取能力，需要实验中的抓取目标种类多样化和位姿多样化。另外，为了能与提出GG-CNN原文献抓取成功率结果相对公平性的横向比较，尽量搭建相同的实验环境和使用相同的抓取目标。


根据算法的实现过程，实验过程中存在的限制条件如下：
\begin{itemize}
\item[（1）]
由于研究中GG-CNN对输入深度图像的尺寸需求和清晰度需求，需要对目标的进行范围裁剪，这时会使用RGB信息确定目标的中心大致范围，所以实验中在平台上铺盖的白纸是必须的。如果有更好的网络能不需要裁剪并稳定指出目标位置，实验环境中白色背景的条件可以被去除。
\item[（2）]
GG-CNN的输出形式决定了如果要使用它的输出结果作为视觉伺服的依据，必须保证相机时刻俯视0°朝下，所以对末端的沿X、Y轴角速度指令不论计算结果如何都会在给机械臂下达命令那一步置零。
\item[（3）]
深度传感器使用的Realsense D435i，它是基于结构光的原理测量深度的，所以对漫反射能力差的目标，获取的深度值几乎是无效值，所以选用的目标都是漫反射效果好的物体。另外，它的深度测量精度为1mm，这决定了选用目标不可以是铅笔等过小的物体，当然，也不可以过大，这会让神经网络无法区分谁是目标谁是背景。
\end{itemize}


实物环境搭建如图\ref{实物环境搭建展示图}，这与GG-CNN原文是类似的。但值得一提的是，经过第四章中对RGB图像和深度图像的特别处理，本文中机械臂的运动范围可以超出平台范围（但初始情况时，目标必须在相机视野范围内），这是相对GG-CNN原文具有优势的地方。根据限制条件（3）和仿照GG-CNN原文使用的抓取目标，选择了六种家常抓取目标用于本次实验的各种测试与验证，它们正如图所示\ref{抓取目标选取}：
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{chapter6/替身}
	\caption{抓取目标选取}
	\label{抓取目标选取}
\end{figure}


抓取目标将以对称的姿态在平台的八个方位的位置摆放，这么做可以很全面地测试伺服系统应对多样化的目标位姿的伺服情况。


六轴JK机械臂初始状态各编码器显示数值为：机械臂末端在$\lbrace$\textit{B}$\rbrace$中的初始状态为：。在这样的初始参数设计下，摄像头获取的图片正好可以将整个平台尽收眼底。
所设定的目标物体坐标系相对于机械臂末端坐标系的相对位姿如表\ref{物体摆放位姿表}所示，相对深度不会改变。
\begin{table}[htbp]
\caption{物体摆放位姿表}
\label{物体摆放位姿表}
\vspace{0.5em}\centering\wuhao
\begin{tabular}{cccc}
\toprule[2pt]
位姿序号 & $\varDelta X$(m) & $\varDelta Y$(m) & $\theta _z$($\degree$)\\
\midrule[1pt]
 1 & 0.3 & 0.3 & 30\\
 2 & 0.3 & -0.3 & -30\\
 3 & 0.3 & 0 & 15\\
 4 & -0.3 & 0.3 & -15\\
 5 & -0.3 & -0.3 & 0\\
 6 & -0.3 & 0 & 30\\
 7 & 0 & 0.3 & -30\\
 8 & 0 & -0.3 & 0\\

\bottomrule[1.5pt]
\end{tabular}
\end{table}


实验中将会在六种不同的物体的八种不同位姿情况下对目标进行抓取，记录的实验结果包括是否成功抓取，伺服超调量和响应时间。响应时间不包括抓物体所需时间，因为抓取是开环控制的，所需时间固定，没有记录的必要。然后会与各个基于生成抓取合成方法来抓取未知物体的文献成果进行比较。最后对不同物体相同位姿以及抓取率百分之百的一个物体不同位姿伺服曲线进行分析，并与同样基于GG-CNN但使用PBVS+IBVS来抓取未知物体的文献成果\cite{haviland2020control}进行比较。
%\section{人机交互界面设计}[Content specification]


\section{实验验证}[Content specification]
\subsection{抓取成功率测试}[Content specification]
对总共48种情况的实验结果进行记录，不仅记录总的抓取成功率，同时记录相同物体的抓取成功率和相同位姿对应不同物体的抓取成功率，绘制表格如表\ref{物体抓取结果记录表}所示：
\begin{table}[htbp]
\caption{物体抓取结果记录表}
\label{物体抓取结果记录表}
\vspace{0.5em}\centering\wuhao
\begin{tabular}{cccccccc}
\toprule[1.5pt]
位姿序号 & 物体1 & 物体2 & 物体3 & 物体4 & 物体5 & 物体6 & 抓取成功率\\
\midrule[1pt]
 1 & 成功 & 成功 & 成功 & 成功 & 成功 & 成功 & 100\%\\
 2 & 成功 & 成功 & 成功 & 成功 & 成功 & 成功 & 100\%\\
 3 & 成功 & 成功 & 成功 & 成功 & 成功 & 成功 & 100\%\\
 4 & 成功 & 成功 & 成功 & 成功 & 成功 & 成功 & 100\%\\
 5 & 成功 & 成功 & 成功 & 成功 & 成功 & 成功 & 100\%\\
 6 & 成功 & 成功 & 成功 & 成功 & 成功 & 成功 & 100\%\\
 7 & 成功 & 成功 & 成功 & 成功 & 失败 & 成功 & 83.3\%\\
 8 & 成功 & 成功 & 成功 & 成功 & 成功 & 失败 & 83.3\%\\

\bottomrule[1.5pt]
抓取成功率 & 100\% & 100\% & 100\% & 100\% & 87.5\% & 87.5\% & \\
\hline
\end{tabular}
\end{table}


将实验获得的数据与各类成抓取合成方法的文献成果在一张表格中进行展现。
\begin{table}[htbp]
\caption{各类成抓取合成成果抓取家常物体成功率对比}
\label{各类成抓取合成成果抓取家常物体成功率对比}
\vspace{0.5em}\centering\wuhao
\begin{tabular}{ccccccc}
\toprule[1.5pt]
   & 文献1\cite{lenz2015deep} & 文献2\cite{pinto2016supersizing} & 文献3\cite{johns2016deep} & 文献4\cite{mahler2017dex} & 文献5\cite{levine2018learning} & 文献6\cite{haviland2020control}\\
\midrule[1pt]
 抓取成功率 & 89\% & 73\% & 80\% & 80\% & 80\% & 92\% \\
 提升 & 6.6\% & 22.6\% & 15.6\% & 15.6\% & 15.6\% & 3.6\% \\

\bottomrule[1.5pt]
\end{tabular}
\end{table}


\subsection{抓取性能验证}[Content specification]
图\ref{比较对象伺服效果}为同样基于GG-CNN但使用PBVS+IBVS来抓取未知物体的伺服过程中特征偏差曲线，他还测量了运动物体的伺服情况，相关的蓝色曲线忽略。本文将只比较他工作中静止物体的伺服对应的红色曲线情况。
\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{chapter6/对比工作伺服效果}
	\caption{比较对象伺服效果}
	\label{比较对象伺服效果}
\end{figure}


选择的目标为，把它不同的摆放位姿情况下的特征偏差平方和均值曲线绘制到同一坐标系下，将不同情况对应的超调量和响应时间绘制到直方图中，如图\ref{相同目标不同位姿抓取性能}所示。
\begin{figure}[h]
	\centering
	\subfigure[特征偏差平方和均值]{
		\begin{minipage}[h]{0.4\textwidth}
			\includegraphics[width=1\textwidth]{chapter6/替身}
		\end{minipage}
		\label{相同目标不同位姿特征偏差平方和均值}
	}
	\subfigure[超调响应时间直方图]{
		\begin{minipage}[h]{0.4\textwidth}
			\includegraphics[width=1\textwidth]{chapter6/替身}
		\end{minipage}
		\label{超调响应时间直方图1}
	}
	\caption{相同目标不同位姿抓取性能}
	\label{相同目标不同位姿抓取性能}
\end{figure}


选择的位姿为。把对应的不同目标情况下的特征偏差曲线绘制到同一坐标系下，将不同情况对应的超调量和响应时间绘制到直方图中，如图\ref{相同位姿不同目标抓取性能}所示：
\begin{figure}[h]
	\centering
	\subfigure[特征偏差平方和均值]{
		\begin{minipage}[h]{0.4\textwidth}
			\includegraphics[width=1\textwidth]{chapter6/替身}
		\end{minipage}
		\label{相同位姿不同目标特征偏差平方和均值}
	}
	\subfigure[超调响应时间直方图]{
		\begin{minipage}[h]{0.4\textwidth}
			\includegraphics[width=1\textwidth]{chapter6/替身}
		\end{minipage}
		\label{超调响应时间直方图2}
	}
	\caption{相同位姿不同目标抓取性能}
	\label{相同位姿不同目标抓取性能}
\end{figure}


\newpage
\section{本章小结}[Content specification]
本章在六种不同家常的物体的八种不同位姿情况下对目标进行抓取实验，在所提出的特征提取、匹配的方法和与之配套的伺服控制律的加持下，在环境限制条件更少的情况下，拥有比许多类似工作的研究成果更高的抓取成功率。与另一个基于GG-CNN抓取未知目标文献成果比较，拥有更平滑而理想的伺服曲线。通过以上实验证明了本研究所提出的未知物体抓取算法有高鲁棒性和高伺服性能。
%\include{body/introduction}

\backmatter
\include{back/conclusion}   % 结论
\bibliographystyle{hithesis} %如果没有参考文献时候
\bibliography{reference}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%-- 注意：以下本硕博、博后书序不一致 --%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% 硕博书序
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%\begin{appendix}%附录
%\input{back/appA.tex}
%\end{appendix}
%\include{back/publications}    % 所发文章
%\include{back/ceindex}    % 索引, 根据自己的情况添加或者不添加，选择自动添加或者手工添加。
%\authorization %授权
%%\authorization[scan.pdf] %添加扫描页的命令，与上互斥
%\include{back/acknowledgements} %致谢
%\include{back/resume}          % 博士学位论文有个人简介
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% 本科书序为：
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% \authorization %授权
%  \authorization[scan.pdf] %添加扫描页的命令，与上互斥
% \include{back/acknowledgements} %致谢

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% 博后书序
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% \include{back/acknowledgements} %致谢
% \include{back/doctorpublications}    % 所发文章
% \include{back/publications}    % 所发文章
% \include{back/resume}          % 博士学位论文有个人简介
% \include{back/correspondingaddr} %通信地址
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\end{document}
% Local Variables:
% TeX-engine: xetex
% End:
